{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mygrad import Tensor\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from mygrad import Tensor\n",
    "from mygrad.nnet.layers import dense\n",
    "from mygrad.nnet.activations import softmax, relu\n",
    "from mygrad.nnet.losses import multiclass_hinge\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mygrad.math import log\n",
    "def cross_entropy(p_pred, p_true):\n",
    "    \"\"\" Computes the mean cross-entropy.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        p_pred : mygrad.Tensor, shape:(N, K)\n",
    "            N predicted distributions, each over K classes.\n",
    "        \n",
    "        p_true : mygrad.Tensor, shape:(N, K)\n",
    "            N 'true' distributions, each over K classes\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=()\n",
    "            The mean cross entropy (scalar).\"\"\"\n",
    "    \n",
    "    N = p_pred.shape[0]\n",
    "    global p_logq\n",
    "    p_logq = (p_true) * log(p_pred)\n",
    "    return (-1/ N) * p_logq.sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.60991107 -0.        ]\n",
      " [-0.         -0.         -1.60958464 -0.         -0.        ]\n",
      " [-0.         -0.         -1.60912031 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.61024727 -0.        ]\n",
      " [-0.         -0.         -0.         -1.60964328 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -1.60893789]]\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor(nan)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p_logq)\n",
    "n = p_logq.sum()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4508, 4508, 4508, 4508, 4508, 5209, 5209, 5209, 5209, 5209, 5589,\n",
       "        5589, 5589, 5589, 5589], dtype=int64),\n",
       " array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4], dtype=int64))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(p_logq.data) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_Movies_and_TV_5.json.gz')\n",
    "# 1697533 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = np.array(df[\"reviewText\"])\n",
    "ratings = np.array(df[\"overall\"])\n",
    "y_train = reviews[:900000]\n",
    "y_val = reviews[900000:1000000]\n",
    "y_test = reviews[1000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9b585e0fcc88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Lilian\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    132\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32mC:\\Users\\Lilian\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 132\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Lilian\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[1;31m# Handles parentheses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Lilian\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[1;31m# internal: pattern.sub/subn implementation helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[1;31m# literal replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "arr = np.zeros(len(reviews), dtype=Counter)\n",
    "i = 0\n",
    "for r in reviews:\n",
    "    tokens = word_tokenize(r)\n",
    "    count = Counter(tokens)\n",
    "    arr[i] = count\n",
    "    i += 1\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#624356\n",
    "reviews = arr[:50000]\n",
    "ratings = np.array(df[\"overall\"])[:50000]\n",
    "y_train = ratings[:20000]\n",
    "y_val = ratings[20000:25000]\n",
    "y_test = ratings[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def to_bag(counters, k=None, stopwords=[]):\n",
    "    instances = Counter()\n",
    "    for counter in counters:\n",
    "        instances += counter\n",
    "    for word in stopwords:\n",
    "        del instances[word]\n",
    "    for p in punctuation:\n",
    "        del instances[p]\n",
    "    if k is not None and k < len(instances):\n",
    "        return list(list(zip(*sorted(instances.most_common(k))))[0])\n",
    "    return sorted(instances)\n",
    "\n",
    "# to_bag(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag = to_bag(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = ratings[:7000]\n",
    "y_val = ratings[7000:9000]\n",
    "y_test = ratings[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag_2 = to_bag(reviews, k=10000, stopwords=stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"''\",\n",
       " \"'90s\",\n",
       " \"'S\",\n",
       " \"'The\",\n",
       " \"'d\",\n",
       " \"'em\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'the\",\n",
       " \"'ve\",\n",
       " '***',\n",
       " '*****',\n",
       " '*The',\n",
       " '--',\n",
       " '..',\n",
       " '...',\n",
       " '.A',\n",
       " '.And',\n",
       " '.As',\n",
       " '.I',\n",
       " '.If',\n",
       " '.In',\n",
       " '.It',\n",
       " '.The',\n",
       " '.This',\n",
       " '.and',\n",
       " '.the',\n",
       " '0',\n",
       " '007',\n",
       " '1',\n",
       " '1.85:1',\n",
       " '1/2',\n",
       " '10',\n",
       " '10/10',\n",
       " '100',\n",
       " '1000',\n",
       " '1080p',\n",
       " '11',\n",
       " '12',\n",
       " '12-year',\n",
       " '12-year-old',\n",
       " '13',\n",
       " '13th',\n",
       " '14',\n",
       " '15',\n",
       " '150',\n",
       " '16',\n",
       " '165',\n",
       " '16:9',\n",
       " '17',\n",
       " '1776',\n",
       " '17th',\n",
       " '18',\n",
       " '18th',\n",
       " '19',\n",
       " '1900',\n",
       " '1903',\n",
       " '1920s',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1936',\n",
       " '1939',\n",
       " '1940',\n",
       " '1941',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '1946',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1951',\n",
       " '1954',\n",
       " '1958',\n",
       " '1959',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1962',\n",
       " '1964',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '1st',\n",
       " '2',\n",
       " '2-Disc',\n",
       " '2-disc',\n",
       " '2.0',\n",
       " '2.35:1',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2013',\n",
       " '20th',\n",
       " '21',\n",
       " '21st',\n",
       " '23',\n",
       " '23rd',\n",
       " '24',\n",
       " '25',\n",
       " '25th',\n",
       " '27',\n",
       " '28',\n",
       " '2nd',\n",
       " '3',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30s',\n",
       " '30th',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '39',\n",
       " '3rd',\n",
       " '4',\n",
       " '4/5',\n",
       " '40',\n",
       " '400',\n",
       " '40s',\n",
       " '40th',\n",
       " '45',\n",
       " '48',\n",
       " '4th',\n",
       " '5',\n",
       " '5-star',\n",
       " '5.1',\n",
       " '50',\n",
       " '50s',\n",
       " '5th',\n",
       " '6',\n",
       " '60',\n",
       " '60s',\n",
       " '62',\n",
       " '640',\n",
       " '65533',\n",
       " '68',\n",
       " '7',\n",
       " '70',\n",
       " '70s',\n",
       " '8',\n",
       " '80',\n",
       " '80s',\n",
       " '8211',\n",
       " '8212',\n",
       " '8216',\n",
       " '8217',\n",
       " '8220',\n",
       " '8221',\n",
       " '9',\n",
       " '9/11',\n",
       " '90',\n",
       " '90210',\n",
       " '90s',\n",
       " '95',\n",
       " '99',\n",
       " 'A',\n",
       " 'A+',\n",
       " 'A-',\n",
       " 'A.',\n",
       " 'AAGPBL',\n",
       " 'ABC',\n",
       " 'ABOUT',\n",
       " 'ACTING',\n",
       " 'AD',\n",
       " 'AFTER',\n",
       " 'AGAIN',\n",
       " 'AIDS',\n",
       " 'AIR',\n",
       " 'ALL',\n",
       " 'AMAZING',\n",
       " 'AMERICAN',\n",
       " 'AN',\n",
       " 'ANACONDA',\n",
       " 'AND',\n",
       " 'ANY',\n",
       " 'ARE',\n",
       " 'AS',\n",
       " 'AT',\n",
       " 'AWESOME',\n",
       " 'Aaron',\n",
       " 'Abbey',\n",
       " 'Abby',\n",
       " 'Abominable',\n",
       " 'About',\n",
       " 'Above',\n",
       " 'Abraham',\n",
       " 'Absence',\n",
       " 'Absolutely',\n",
       " 'Academy',\n",
       " 'According',\n",
       " 'Ace',\n",
       " 'Ackroyd',\n",
       " 'Acting',\n",
       " 'Action',\n",
       " 'Actor',\n",
       " 'Actors',\n",
       " 'Actress',\n",
       " 'Acts',\n",
       " 'Actually',\n",
       " 'Adam',\n",
       " 'Adams',\n",
       " 'Add',\n",
       " 'Additionally',\n",
       " 'Admittedly',\n",
       " 'Ado',\n",
       " 'Adventures',\n",
       " 'Africa',\n",
       " 'African',\n",
       " 'After',\n",
       " 'Again',\n",
       " 'Agatha',\n",
       " 'Age',\n",
       " 'Agent',\n",
       " 'Agnes',\n",
       " 'Ah',\n",
       " 'Aidan',\n",
       " 'Aiello',\n",
       " 'Air',\n",
       " 'Aires',\n",
       " 'Akira',\n",
       " 'Akkad',\n",
       " 'Al',\n",
       " 'Alan',\n",
       " 'Alas',\n",
       " 'Albert',\n",
       " 'Alec',\n",
       " 'Alex',\n",
       " 'Alexander',\n",
       " 'Alfred',\n",
       " 'Alice',\n",
       " 'Alicia',\n",
       " 'Alien',\n",
       " 'Aliens',\n",
       " 'All',\n",
       " 'All-American',\n",
       " 'Allen',\n",
       " 'Alley',\n",
       " 'Allgood',\n",
       " 'Allie',\n",
       " 'Allied',\n",
       " 'Allies',\n",
       " 'Allison',\n",
       " 'Almost',\n",
       " 'Alone',\n",
       " 'Along',\n",
       " 'Also',\n",
       " 'Although',\n",
       " 'Always',\n",
       " 'Am',\n",
       " 'Amazing',\n",
       " 'Amazon',\n",
       " 'Amazon.com',\n",
       " 'Amelia',\n",
       " 'America',\n",
       " 'American',\n",
       " 'Americans',\n",
       " 'Among',\n",
       " 'An',\n",
       " 'Anaconda',\n",
       " 'Anacondas',\n",
       " 'Anchor',\n",
       " 'And',\n",
       " 'Andie',\n",
       " 'Andrew',\n",
       " 'Andrews',\n",
       " 'Andy',\n",
       " 'Angel',\n",
       " 'Angeles',\n",
       " 'Angels',\n",
       " 'Anguish',\n",
       " 'Animagic',\n",
       " 'Animal',\n",
       " 'Animals',\n",
       " 'Ann',\n",
       " 'Anna',\n",
       " 'Annaud',\n",
       " 'Anne',\n",
       " 'Annelle',\n",
       " 'Annis',\n",
       " 'Anniversary',\n",
       " 'Another',\n",
       " 'Anthony',\n",
       " 'Anti-Semitic',\n",
       " 'Antietam',\n",
       " 'Anton',\n",
       " 'Antonio',\n",
       " 'Anxiety',\n",
       " 'Any',\n",
       " 'Anyone',\n",
       " 'Anything',\n",
       " 'Anyway',\n",
       " 'Anyways',\n",
       " 'Apart',\n",
       " 'Apollo',\n",
       " 'Apostles',\n",
       " 'Apparently',\n",
       " 'April',\n",
       " 'Arab',\n",
       " 'Arabic',\n",
       " 'Arabs',\n",
       " 'Aramaic',\n",
       " 'Arc',\n",
       " 'Are',\n",
       " 'Argentina',\n",
       " 'Argento',\n",
       " 'Arizona',\n",
       " 'Arkin',\n",
       " 'Army',\n",
       " 'Arnold',\n",
       " 'Arrow',\n",
       " 'Art',\n",
       " 'Arthur',\n",
       " 'As',\n",
       " 'Ashley',\n",
       " 'Asia',\n",
       " 'Asian',\n",
       " 'Aside',\n",
       " 'Astaire',\n",
       " 'At',\n",
       " 'Atkins',\n",
       " 'Atlantic',\n",
       " 'Attorney',\n",
       " 'Audio',\n",
       " 'Aufschnaiter',\n",
       " 'Austen',\n",
       " 'Austin',\n",
       " 'Australian',\n",
       " 'Austria',\n",
       " 'Austrian',\n",
       " 'Avengers',\n",
       " 'Avenue',\n",
       " 'Avoid',\n",
       " 'Award',\n",
       " 'Awards',\n",
       " 'Awesome',\n",
       " 'Aykroyd',\n",
       " 'B',\n",
       " 'B+',\n",
       " 'B-movie',\n",
       " 'BAD',\n",
       " 'BBC',\n",
       " 'BD',\n",
       " 'BD-Live',\n",
       " 'BE',\n",
       " 'BECAUSE',\n",
       " 'BEFORE',\n",
       " 'BEST',\n",
       " 'BETTER',\n",
       " 'BIBLE',\n",
       " 'BIG',\n",
       " 'BLACK',\n",
       " 'BLU-RAY',\n",
       " 'BOTH',\n",
       " 'BRASCO',\n",
       " 'BTW',\n",
       " 'BUT',\n",
       " 'BUY',\n",
       " 'BY',\n",
       " 'Baby',\n",
       " 'Back',\n",
       " 'Bad',\n",
       " 'Baker',\n",
       " 'Bakshi',\n",
       " 'Balaban',\n",
       " 'Ball',\n",
       " 'Banderas',\n",
       " 'Bandits',\n",
       " 'Barbara',\n",
       " 'Bard',\n",
       " 'Barney',\n",
       " 'Baron',\n",
       " 'Barry',\n",
       " 'Barrymore',\n",
       " 'Barton',\n",
       " 'Baseball',\n",
       " 'Based',\n",
       " 'Basic',\n",
       " 'Basically',\n",
       " 'Basinger',\n",
       " 'Bass',\n",
       " 'Bates',\n",
       " 'Batman',\n",
       " 'Battle',\n",
       " 'Battra',\n",
       " 'Bava',\n",
       " 'Bay',\n",
       " 'Be',\n",
       " 'Bear',\n",
       " 'Beatles',\n",
       " 'Beatrice',\n",
       " 'Beautiful',\n",
       " 'Beautifully',\n",
       " 'Because',\n",
       " 'Beckett',\n",
       " 'Beckinsale',\n",
       " 'Becky',\n",
       " 'Bedouin',\n",
       " 'Beetlejuice',\n",
       " 'Before',\n",
       " 'Behind',\n",
       " 'Being',\n",
       " 'Believe',\n",
       " 'Bell',\n",
       " 'Bellamy',\n",
       " 'Bellamys',\n",
       " 'Bellucci',\n",
       " 'Ben',\n",
       " 'Benedick',\n",
       " 'Benedict',\n",
       " 'Benicio',\n",
       " 'Benjamin',\n",
       " 'Bennett',\n",
       " 'Benny',\n",
       " 'Berkeley',\n",
       " 'Berlin',\n",
       " 'Bernard',\n",
       " 'Berthold',\n",
       " 'Besides',\n",
       " 'Besson',\n",
       " 'Best',\n",
       " 'Better',\n",
       " 'Betty',\n",
       " 'Between',\n",
       " 'Beverly',\n",
       " 'Beyond',\n",
       " 'Bible',\n",
       " 'Biblical',\n",
       " 'Biblically',\n",
       " 'Big',\n",
       " 'Bill',\n",
       " 'Billie',\n",
       " 'Billy',\n",
       " 'Bird',\n",
       " 'Black',\n",
       " 'Blackman',\n",
       " 'Blade',\n",
       " 'Blanche',\n",
       " 'Blazing',\n",
       " 'Bless',\n",
       " 'Blessed',\n",
       " 'Blockbuster',\n",
       " 'Blood',\n",
       " 'Blu',\n",
       " 'Blu-Ray',\n",
       " 'Blu-ray',\n",
       " 'BluRay',\n",
       " 'Blue',\n",
       " 'Bluray',\n",
       " 'Bo',\n",
       " 'Board',\n",
       " 'Boat',\n",
       " 'Bob',\n",
       " 'Bobby',\n",
       " 'Body',\n",
       " 'Bogart',\n",
       " 'Bogie',\n",
       " 'Bond',\n",
       " 'Bonjour',\n",
       " 'Book',\n",
       " 'Books',\n",
       " 'Boot',\n",
       " 'Borgnine',\n",
       " 'Born',\n",
       " 'Both',\n",
       " 'Bottom',\n",
       " 'Boudreaux',\n",
       " 'Bought',\n",
       " 'Box',\n",
       " 'Boy',\n",
       " 'Boys',\n",
       " 'Brad',\n",
       " 'Brain',\n",
       " 'Branagh',\n",
       " 'Brando',\n",
       " 'Brannagh',\n",
       " 'Brasco',\n",
       " 'Brave',\n",
       " 'Braveheart',\n",
       " 'Bravo',\n",
       " 'Brazil',\n",
       " 'Brent',\n",
       " 'Brian',\n",
       " 'Bride',\n",
       " 'Bridges',\n",
       " 'Bridget',\n",
       " 'Briers',\n",
       " 'Brilliant',\n",
       " 'Brimley',\n",
       " 'Brion',\n",
       " 'Brit',\n",
       " 'Britain',\n",
       " 'British',\n",
       " 'Brits',\n",
       " 'Broadway',\n",
       " 'Brolin',\n",
       " 'Bronte',\n",
       " 'Brooklyn',\n",
       " 'Brooks',\n",
       " 'Brother',\n",
       " 'Brothers',\n",
       " 'Brown',\n",
       " 'Bruce',\n",
       " 'Bruno',\n",
       " 'Buck',\n",
       " 'Buckley',\n",
       " 'Buddhist',\n",
       " 'Buenos',\n",
       " 'Bug',\n",
       " 'Bugs',\n",
       " 'Bumble',\n",
       " 'Burgermeister',\n",
       " 'Burke',\n",
       " 'Burl',\n",
       " 'Burton',\n",
       " 'Busey',\n",
       " 'Bush',\n",
       " 'But',\n",
       " 'Butler',\n",
       " 'Buy',\n",
       " 'By',\n",
       " 'C',\n",
       " \"C'mon\",\n",
       " 'C.',\n",
       " 'CAINE',\n",
       " 'CAN',\n",
       " 'CAR',\n",
       " 'CBS',\n",
       " 'CD',\n",
       " 'CE3K',\n",
       " 'CG',\n",
       " 'CGI',\n",
       " 'CHRIST',\n",
       " 'CHRISTMAS',\n",
       " 'CIA',\n",
       " 'CLASSIC',\n",
       " 'CLOSE',\n",
       " 'COULARDEAU',\n",
       " 'COULD',\n",
       " 'Ca',\n",
       " 'Caesar',\n",
       " 'Cage',\n",
       " 'Caiaphas',\n",
       " 'Caine',\n",
       " 'Caiphas',\n",
       " 'Caleb',\n",
       " 'Calvary',\n",
       " 'Came',\n",
       " 'Cameron',\n",
       " 'Campbell',\n",
       " 'Can',\n",
       " 'Canada',\n",
       " 'Capt',\n",
       " 'Captain',\n",
       " 'Car',\n",
       " 'Carl',\n",
       " 'Carlisle',\n",
       " 'Carlos',\n",
       " 'Carlson',\n",
       " 'Carmen',\n",
       " 'Carol',\n",
       " 'Carson',\n",
       " 'Carter',\n",
       " 'Cary',\n",
       " 'Casablanca',\n",
       " 'Casper',\n",
       " 'Cassius',\n",
       " 'Cast',\n",
       " 'Castle',\n",
       " 'Cat',\n",
       " 'Catherine',\n",
       " 'Catholic',\n",
       " 'Catholicism',\n",
       " 'Catholics',\n",
       " 'Cathy',\n",
       " 'Caveziel',\n",
       " 'Caviezel',\n",
       " 'Cecile',\n",
       " 'Celentano',\n",
       " 'Celtic',\n",
       " 'Century',\n",
       " 'Certainly',\n",
       " 'Chad',\n",
       " 'Chairs',\n",
       " 'Chambers',\n",
       " 'Channel',\n",
       " 'Character',\n",
       " 'Charles',\n",
       " 'Charlie',\n",
       " 'Charlotte',\n",
       " 'Check',\n",
       " 'Chesnick',\n",
       " 'Chicago',\n",
       " 'Chief',\n",
       " 'Child',\n",
       " 'Children',\n",
       " 'China',\n",
       " 'Chinese',\n",
       " 'Chris',\n",
       " 'Christ',\n",
       " 'Christian',\n",
       " 'Christianity',\n",
       " 'Christians',\n",
       " 'Christie',\n",
       " 'Christina',\n",
       " 'Christine',\n",
       " 'Christmas',\n",
       " 'Christmastime',\n",
       " 'Christmastown',\n",
       " 'Christopher',\n",
       " 'Church',\n",
       " 'Churchill',\n",
       " 'Ciaran',\n",
       " 'Cinema',\n",
       " 'Cinematography',\n",
       " 'City',\n",
       " 'Civil',\n",
       " 'Claire',\n",
       " 'Clairee',\n",
       " 'Clancy',\n",
       " 'Clarice',\n",
       " 'Clark',\n",
       " 'Clarke',\n",
       " 'Classic',\n",
       " 'Classics',\n",
       " 'Claude',\n",
       " 'Claudio',\n",
       " 'Claus',\n",
       " 'Clause',\n",
       " 'Clavell',\n",
       " 'Clay',\n",
       " 'Clearly',\n",
       " 'Clinton',\n",
       " 'Cloris',\n",
       " 'Close',\n",
       " 'Club',\n",
       " 'Code',\n",
       " 'Cold',\n",
       " 'Colin',\n",
       " 'Collection',\n",
       " 'Collector',\n",
       " 'Collins',\n",
       " 'Color',\n",
       " 'Colors',\n",
       " 'Columbia',\n",
       " 'Come',\n",
       " 'Comedy',\n",
       " 'Comin',\n",
       " 'Coming',\n",
       " 'Commander',\n",
       " 'Commandments',\n",
       " 'Commentary',\n",
       " 'Commodores',\n",
       " 'Communism',\n",
       " 'Communist',\n",
       " 'Company',\n",
       " 'Compare',\n",
       " 'Compared',\n",
       " 'Complete',\n",
       " 'Condorman',\n",
       " 'Connelly',\n",
       " 'Considering',\n",
       " 'Contact',\n",
       " 'Continental',\n",
       " 'Control',\n",
       " 'Cook',\n",
       " 'Cooke',\n",
       " 'Cool',\n",
       " 'Cooper',\n",
       " 'Cop',\n",
       " 'Corbin',\n",
       " 'Corey',\n",
       " 'Corinne',\n",
       " 'Cornelius',\n",
       " 'Corvino',\n",
       " 'Cosmos',\n",
       " 'Could',\n",
       " 'Count',\n",
       " 'Couple',\n",
       " 'Court',\n",
       " 'Cox',\n",
       " 'Craig',\n",
       " 'Crawford',\n",
       " 'Crawley',\n",
       " 'Creepers',\n",
       " 'Crewson',\n",
       " 'Crimson',\n",
       " 'Criterion',\n",
       " 'Critics',\n",
       " 'Cross',\n",
       " 'Crossing',\n",
       " 'Crucifixion',\n",
       " 'Crystal',\n",
       " 'Cuba',\n",
       " 'Cube',\n",
       " 'Culkin',\n",
       " 'Cusack',\n",
       " 'Cut',\n",
       " 'Cute',\n",
       " 'Cyrene',\n",
       " 'D',\n",
       " \"D'Onofrio\",\n",
       " \"D'Urbervilles\",\n",
       " 'D.',\n",
       " 'DA',\n",
       " 'DAY',\n",
       " 'DD',\n",
       " 'DEA',\n",
       " 'DEMONS',\n",
       " 'DID',\n",
       " 'DIFFERENT',\n",
       " 'DJ',\n",
       " 'DNA',\n",
       " 'DNR',\n",
       " 'DO',\n",
       " 'DOES',\n",
       " 'DONE',\n",
       " 'DONNIE',\n",
       " 'DTS',\n",
       " 'DTS-HD',\n",
       " 'DVD',\n",
       " 'DVD.The',\n",
       " 'DVDs',\n",
       " 'Dad',\n",
       " 'Dalai',\n",
       " 'Dali',\n",
       " 'Dallas',\n",
       " 'Dalton',\n",
       " 'Damme',\n",
       " 'Dan',\n",
       " 'Dance',\n",
       " 'Danes',\n",
       " 'Danger',\n",
       " 'Daniel',\n",
       " 'Daniels',\n",
       " 'Danny',\n",
       " 'Daria',\n",
       " 'Dario',\n",
       " 'Dark',\n",
       " 'Darlington',\n",
       " 'Darrell',\n",
       " 'Darren',\n",
       " 'Darryl',\n",
       " 'Daryl',\n",
       " 'Das',\n",
       " 'David',\n",
       " 'Davies',\n",
       " 'Davis',\n",
       " 'Dawn',\n",
       " 'Day',\n",
       " 'Days',\n",
       " 'De',\n",
       " 'Dead',\n",
       " 'Dean',\n",
       " 'Death',\n",
       " 'Debney',\n",
       " 'Deborah',\n",
       " 'Debra',\n",
       " 'December',\n",
       " 'Dedee',\n",
       " 'DeeDee',\n",
       " 'Deedee',\n",
       " 'Deep',\n",
       " 'Def',\n",
       " 'Defense',\n",
       " 'Definitely',\n",
       " 'Definition',\n",
       " 'Del',\n",
       " 'Delaware',\n",
       " 'Deleted',\n",
       " 'Deluxe',\n",
       " 'Demme',\n",
       " 'Demons',\n",
       " 'Denise',\n",
       " 'Dennis',\n",
       " 'Denying',\n",
       " 'Denzel',\n",
       " 'Depp',\n",
       " 'Depression',\n",
       " 'Dermot',\n",
       " 'Deschanel',\n",
       " 'Desert',\n",
       " 'Despite',\n",
       " 'Detective',\n",
       " 'Detillo',\n",
       " 'Devil',\n",
       " 'Diamond',\n",
       " 'Diana',\n",
       " 'Diary',\n",
       " 'Diaz',\n",
       " 'Dick',\n",
       " 'Dickens',\n",
       " 'Did',\n",
       " 'Die',\n",
       " 'Dien',\n",
       " 'Dies',\n",
       " 'Different',\n",
       " 'Digital',\n",
       " 'Dillon',\n",
       " 'Dina',\n",
       " 'Directed',\n",
       " 'Director',\n",
       " 'Directors',\n",
       " 'Disc',\n",
       " 'Disco',\n",
       " 'Disney',\n",
       " 'Distance',\n",
       " 'Diva',\n",
       " 'Divas',\n",
       " 'Dizzy',\n",
       " 'Dmytryk',\n",
       " 'Do',\n",
       " 'Dobbin',\n",
       " 'Doctor',\n",
       " 'Does',\n",
       " 'Dogberry',\n",
       " 'Dogs',\n",
       " 'Dolby',\n",
       " 'Dolly',\n",
       " 'Dolorous',\n",
       " 'Dolph',\n",
       " 'Dom',\n",
       " 'Don',\n",
       " 'Donald',\n",
       " 'Donna',\n",
       " 'Donnie',\n",
       " 'Donovan',\n",
       " 'Doogie',\n",
       " 'Doris',\n",
       " 'Dorothy',\n",
       " 'Dottie',\n",
       " 'Double',\n",
       " 'Doug',\n",
       " 'Douglas',\n",
       " 'Down',\n",
       " 'Downey',\n",
       " 'Downstairs',\n",
       " 'Downton',\n",
       " 'Doyle',\n",
       " 'Dr',\n",
       " 'Dr.',\n",
       " 'Dracula',\n",
       " 'Drama',\n",
       " 'Dream',\n",
       " 'Dreams',\n",
       " 'Dreyfus',\n",
       " 'Dreyfuss',\n",
       " 'Driver',\n",
       " 'Drum',\n",
       " 'Drummer',\n",
       " 'Duchamp',\n",
       " 'Due',\n",
       " 'Duel',\n",
       " 'Dugan',\n",
       " 'Dukakis',\n",
       " 'Duke',\n",
       " 'Dunne',\n",
       " 'Durante',\n",
       " 'During',\n",
       " 'Dutch',\n",
       " 'Duvall',\n",
       " 'Dylan',\n",
       " 'E',\n",
       " 'E.',\n",
       " 'E.G',\n",
       " 'E.T',\n",
       " 'EDITION',\n",
       " 'ELEMENT',\n",
       " 'ENCOUNTERS',\n",
       " 'END',\n",
       " 'ENJOY',\n",
       " 'ET',\n",
       " 'EVEN',\n",
       " 'EVER',\n",
       " 'EVERY',\n",
       " 'EVERYONE',\n",
       " 'EXCELLENT',\n",
       " 'Each',\n",
       " 'Earth',\n",
       " 'East',\n",
       " 'Easter',\n",
       " 'Eastern',\n",
       " 'Eastland',\n",
       " 'Eatenton',\n",
       " 'Eaton',\n",
       " 'Ebert',\n",
       " 'Eckhart',\n",
       " 'Ed',\n",
       " 'Eddie',\n",
       " 'Edgar',\n",
       " 'Edgeware',\n",
       " 'Edgware',\n",
       " 'Edition',\n",
       " 'Edward',\n",
       " 'Edwardian',\n",
       " 'Edwards',\n",
       " 'Effects',\n",
       " 'Egypt',\n",
       " 'Egyptian',\n",
       " 'Eiji',\n",
       " 'Eileen',\n",
       " 'Either',\n",
       " 'Element',\n",
       " 'Elf',\n",
       " 'Elizabeth',\n",
       " 'Ellen',\n",
       " 'Elmo',\n",
       " 'Elsa',\n",
       " 'Elton',\n",
       " 'Elvis',\n",
       " 'Emily',\n",
       " 'Emma',\n",
       " 'Emmerich',\n",
       " 'Empire',\n",
       " 'Encounters',\n",
       " 'End',\n",
       " 'Enemy',\n",
       " 'England',\n",
       " 'English',\n",
       " 'Enjoy',\n",
       " 'Enjoyed',\n",
       " 'Enough',\n",
       " 'Ensign',\n",
       " 'Enter',\n",
       " 'Entertaining',\n",
       " 'Entertainment',\n",
       " 'Episode',\n",
       " 'Equally',\n",
       " 'Era',\n",
       " 'Eric',\n",
       " 'Ernest',\n",
       " 'Ernie',\n",
       " 'Especially',\n",
       " 'Ethan',\n",
       " 'Ethel',\n",
       " 'Eugene',\n",
       " 'Euro',\n",
       " 'Europe',\n",
       " 'European',\n",
       " 'Eve',\n",
       " 'Even',\n",
       " 'Eventually',\n",
       " 'Ever',\n",
       " 'Everett',\n",
       " 'Every',\n",
       " 'Everybody',\n",
       " 'Everyone',\n",
       " 'Everything',\n",
       " 'Evil',\n",
       " 'Ewan',\n",
       " 'Excellent',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_tf(counts, bow):\n",
    "    desc = []\n",
    "    for word in bow:\n",
    "        desc.append(counts[word])\n",
    "    desc = np.array(desc)\n",
    "    desc[np.where(desc is None)] = 0\n",
    "    if sum(desc) != 0:\n",
    "        des = desc / sum(desc)\n",
    "    else:\n",
    "        des = desc\n",
    "    return des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfs = [to_tf(r, bag_2) for r in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def idf(bow, counts):\n",
    "    idfs = []\n",
    "    total = len(counts)\n",
    "    for word in bow:\n",
    "        idfs.append(total / np.count_nonzero([g[word] for g in counts]))\n",
    "    return np.log10(np.array(idfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(tf, idf):\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idfs = idf(bag_2, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_dist(a, b):\n",
    "    abd = a.dot(b)\n",
    "    mag_a = np.sqrt(np.sum(a ** 2))\n",
    "    mag_b = np.sqrt(np.sum(b ** 2))\n",
    "    return (abd / (mag_a * mag_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf = tfidf(tfs, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = tf_idf\n",
    "x_train = inputs[:7000]\n",
    "x_val = inputs[7000:9000]\n",
    "x_test = inputs[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4508, 4508, 4508, ..., 9208, 9208, 9208], dtype=int64),\n",
       " array([   0,    1,    2, ..., 9997, 9998, 9999], dtype=int64))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(tfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model_out, labels):\n",
    "    \"\"\" Computes the mean accuracy, given predictions and true-labels.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model_out : numpy.ndarray, shape=(N, K)\n",
    "            The predicted class-scores/probabilities\n",
    "        labels : numpy.ndarray, shape=(N,)\n",
    "            The labels for the data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The mean classification accuracy of the N samples.\"\"\"\n",
    "    return np.mean(np.argmax(model_out, axis=1) == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(param, rate):\n",
    "    \"\"\" Performs a gradient-descent update on the parameter.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        param : mygrad.Tensor\n",
    "            The parameter to be updated.\n",
    "        \n",
    "        rate : float\n",
    "            The step size used in the update\"\"\"\n",
    "    param.data -= rate*param.grad\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def he_normal(shape):\n",
    "    \"\"\" Given the desired shape of your array, draws random\n",
    "        values from a scaled-Gaussian distribution.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\"\"\"\n",
    "    N = shape[0]\n",
    "    scale = 1 / np.sqrt(2*N)\n",
    "    return np.random.randn(*shape)#*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"save_data.txt\", \"rb\") as f:\n",
    "    reviews, ratings, inputs, bag_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"save_data.txt\", \"wb\") as f:\n",
    "    pickle.dump([reviews, ratings, inputs, bag_2], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02067323,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.00800254,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x, y, xval, yval, step=1, iters=400):\n",
    "    w1 = Tensor(he_normal((x.shape[1], 2000)))\n",
    "    print(w1[0, 0])\n",
    "    b1 = Tensor(np.zeros(2000, dtype=w1.dtype))\n",
    "    w2 = Tensor(he_normal((2000, 300)))\n",
    "    b2 = Tensor(np.zeros(300, dtype=w2.dtype))\n",
    "    w3 = Tensor(he_normal((300, 5)))\n",
    "    b3 = Tensor(np.zeros(5, dtype=w3.dtype))\n",
    "    params = [b1, w1, b2, w2, b3, w3]\n",
    "\n",
    "    y_cross = np.zeros((x.shape[0], 5))\n",
    "    y_cross[np.arange(x.shape[0]), y] = 1\n",
    "    \n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    t0 = time.time()\n",
    "    for i in range(iters):\n",
    "        if i % 10 == 0:\n",
    "            print(i, time.time() - t0)\n",
    "        o1 = relu(dense(x, w1) + b1)\n",
    "        o2 = relu(dense(o1, w2) + b2)\n",
    "        y_pred = softmax(dense(o2, w3) + b3)\n",
    "        print(np.where(y_pred < 0))\n",
    "        \n",
    "        loss = cross_entropy(y_pred, y_cross)\n",
    "        losses.append(loss.data.item())\n",
    "        loss.backward()\n",
    "\n",
    "        accuracies.append(compute_accuracy(y_pred.data, y))\n",
    "\n",
    "        for param in params:\n",
    "            sgd(param, step)\n",
    "        print(w1[0, 0])\n",
    "\n",
    "        loss.null_gradients()\n",
    "    \n",
    "    o1 = relu(dense(x, w1) + b1)\n",
    "    o2 = relu(dense(o1, w2) + b2)\n",
    "    train_pred = softmax(dense(o2, w3) + b3)\n",
    "    train_acc = compute_accuracy(train_pred, y)\n",
    "    \n",
    "    o1 = relu(dense(xval, w1) + b1)\n",
    "    o2 = relu(dense(o1, w2) + b2)\n",
    "    val_pred = softmax(dense(o2, w3) + b3)\n",
    "    val_acc = compute_accuracy(val_pred, yval)\n",
    "    print(\"Training accuracy:\", train_acc, \"; Validation accuracy:\", val_acc)\n",
    "    return train_acc, val_acc, accuracies, losses, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(0.07294358180734294)\n",
      "0 0.0\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid log-domain value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-170a25823a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mva\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-153-1c856aef954c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, y, xval, yval, step, iters)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_cross\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-c3e75331c339>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(p_pred, p_true)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mp_logq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mp_logq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp_true\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mp_logq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\lilian\\cogworks\\mygrad2\\mygrad\\mygrad\\math.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         mygrad.Tensor\"\"\"\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\lilian\\cogworks\\mygrad2\\mygrad\\mygrad\\tensor_base.py\u001b[0m in \u001b[0;36m_op\u001b[0;34m(cls, Op, op_args, op_kwargs, *input_vars)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mop_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtensor_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mop_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mop_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[1;31m# record that a variable participated in that op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\lilian\\cogworks\\mygrad2\\mygrad\\mygrad\\operations\\log.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid log-domain value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid log-domain value"
     ]
    }
   ],
   "source": [
    "ta, va, acc, los, pms = train(x_train, y_train, x_val, y_val, iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38514285714285712,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
