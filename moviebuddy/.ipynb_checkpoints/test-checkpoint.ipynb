{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mygrad import Tensor\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from mygrad import Tensor\n",
    "from mygrad.nnet.layers import dense\n",
    "from mygrad.nnet.activations import softmax, relu\n",
    "from mygrad.nnet.losses import multiclass_hinge\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mygrad.math import log\n",
    "def cross_entropy(p_pred, p_true):\n",
    "    \"\"\" Computes the mean cross-entropy.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        p_pred : mygrad.Tensor, shape:(N, K)\n",
    "            N predicted distributions, each over K classes.\n",
    "        \n",
    "        p_true : mygrad.Tensor, shape:(N, K)\n",
    "            N 'true' distributions, each over K classes\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=()\n",
    "            The mean cross entropy (scalar).\"\"\"\n",
    "    \n",
    "    N = p_pred.shape[0]\n",
    "    print(p_pred, p_true)\n",
    "    global p_logq\n",
    "    p_logq = (p_true) * log(p_pred)\n",
    "    print(p_logq)\n",
    "    print(p_logq.sum())\n",
    "    return (-1/ N) * p_logq.sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.60991107 -0.        ]\n",
      " [-0.         -0.         -1.60958464 -0.         -0.        ]\n",
      " [-0.         -0.         -1.60912031 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.61024727 -0.        ]\n",
      " [-0.         -0.         -0.         -1.60964328 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -1.60893789]]\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor(nan)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p_logq)\n",
    "n = p_logq.sum()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4508, 4508, 4508, 4508, 4508, 5209, 5209, 5209, 5209, 5209, 5589,\n",
       "        5589, 5589, 5589, 5589], dtype=int64),\n",
       " array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4], dtype=int64))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(p_logq.data) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_Movies_and_TV_5.json.gz')\n",
    "# 1697533 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = np.array(df[\"reviewText\"])\n",
    "ratings = np.array(df[\"overall\"])\n",
    "y_train = reviews[:900000]\n",
    "y_val = reviews[900000:1000000]\n",
    "y_test = reviews[1000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9b585e0fcc88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Lilian\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    132\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32mC:\\Users\\Lilian\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 132\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Lilian\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[1;31m# Handles parentheses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Lilian\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[1;31m# internal: pattern.sub/subn implementation helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[1;31m# literal replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "arr = np.zeros(len(reviews), dtype=Counter)\n",
    "i = 0\n",
    "for r in reviews:\n",
    "    tokens = word_tokenize(r)\n",
    "    count = Counter(tokens)\n",
    "    arr[i] = count\n",
    "    i += 1\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#624356\n",
    "reviews = arr[:50000]\n",
    "ratings = np.array(df[\"overall\"])[:50000]\n",
    "y_train = ratings[:20000]\n",
    "y_val = ratings[20000:25000]\n",
    "y_test = ratings[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def to_bag(counters, k=None, stopwords=[]):\n",
    "    instances = Counter()\n",
    "    for counter in counters:\n",
    "        instances += counter\n",
    "    for word in stopwords:\n",
    "        del instances[word]\n",
    "    for p in punctuation:\n",
    "        del instances[p]\n",
    "    if k is not None and k < len(instances):\n",
    "        return list(list(zip(*sorted(instances.most_common(k))))[0])\n",
    "    return sorted(instances)\n",
    "\n",
    "# to_bag(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag = to_bag(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = ratings[:7000]\n",
    "y_val = ratings[7000:9000]\n",
    "y_test = ratings[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag_2 = to_bag(reviews, k=10000, stopwords=stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"''\",\n",
       " \"'90s\",\n",
       " \"'S\",\n",
       " \"'The\",\n",
       " \"'d\",\n",
       " \"'em\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'the\",\n",
       " \"'ve\",\n",
       " '***',\n",
       " '*****',\n",
       " '*The',\n",
       " '--',\n",
       " '..',\n",
       " '...',\n",
       " '.A',\n",
       " '.And',\n",
       " '.As',\n",
       " '.I',\n",
       " '.If',\n",
       " '.In',\n",
       " '.It',\n",
       " '.The',\n",
       " '.This',\n",
       " '.and',\n",
       " '.the',\n",
       " '0',\n",
       " '007',\n",
       " '1',\n",
       " '1.85:1',\n",
       " '1/2',\n",
       " '10',\n",
       " '10/10',\n",
       " '100',\n",
       " '1000',\n",
       " '1080p',\n",
       " '11',\n",
       " '12',\n",
       " '12-year',\n",
       " '12-year-old',\n",
       " '13',\n",
       " '13th',\n",
       " '14',\n",
       " '15',\n",
       " '150',\n",
       " '16',\n",
       " '165',\n",
       " '16:9',\n",
       " '17',\n",
       " '1776',\n",
       " '17th',\n",
       " '18',\n",
       " '18th',\n",
       " '19',\n",
       " '1900',\n",
       " '1903',\n",
       " '1920s',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1936',\n",
       " '1939',\n",
       " '1940',\n",
       " '1941',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '1946',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1951',\n",
       " '1954',\n",
       " '1958',\n",
       " '1959',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1962',\n",
       " '1964',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '1st',\n",
       " '2',\n",
       " '2-Disc',\n",
       " '2-disc',\n",
       " '2.0',\n",
       " '2.35:1',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2013',\n",
       " '20th',\n",
       " '21',\n",
       " '21st',\n",
       " '23',\n",
       " '23rd',\n",
       " '24',\n",
       " '25',\n",
       " '25th',\n",
       " '27',\n",
       " '28',\n",
       " '2nd',\n",
       " '3',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30s',\n",
       " '30th',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '39',\n",
       " '3rd',\n",
       " '4',\n",
       " '4/5',\n",
       " '40',\n",
       " '400',\n",
       " '40s',\n",
       " '40th',\n",
       " '45',\n",
       " '48',\n",
       " '4th',\n",
       " '5',\n",
       " '5-star',\n",
       " '5.1',\n",
       " '50',\n",
       " '50s',\n",
       " '5th',\n",
       " '6',\n",
       " '60',\n",
       " '60s',\n",
       " '62',\n",
       " '640',\n",
       " '65533',\n",
       " '68',\n",
       " '7',\n",
       " '70',\n",
       " '70s',\n",
       " '8',\n",
       " '80',\n",
       " '80s',\n",
       " '8211',\n",
       " '8212',\n",
       " '8216',\n",
       " '8217',\n",
       " '8220',\n",
       " '8221',\n",
       " '9',\n",
       " '9/11',\n",
       " '90',\n",
       " '90210',\n",
       " '90s',\n",
       " '95',\n",
       " '99',\n",
       " 'A',\n",
       " 'A+',\n",
       " 'A-',\n",
       " 'A.',\n",
       " 'AAGPBL',\n",
       " 'ABC',\n",
       " 'ABOUT',\n",
       " 'ACTING',\n",
       " 'AD',\n",
       " 'AFTER',\n",
       " 'AGAIN',\n",
       " 'AIDS',\n",
       " 'AIR',\n",
       " 'ALL',\n",
       " 'AMAZING',\n",
       " 'AMERICAN',\n",
       " 'AN',\n",
       " 'ANACONDA',\n",
       " 'AND',\n",
       " 'ANY',\n",
       " 'ARE',\n",
       " 'AS',\n",
       " 'AT',\n",
       " 'AWESOME',\n",
       " 'Aaron',\n",
       " 'Abbey',\n",
       " 'Abby',\n",
       " 'Abominable',\n",
       " 'About',\n",
       " 'Above',\n",
       " 'Abraham',\n",
       " 'Absence',\n",
       " 'Absolutely',\n",
       " 'Academy',\n",
       " 'According',\n",
       " 'Ace',\n",
       " 'Ackroyd',\n",
       " 'Acting',\n",
       " 'Action',\n",
       " 'Actor',\n",
       " 'Actors',\n",
       " 'Actress',\n",
       " 'Acts',\n",
       " 'Actually',\n",
       " 'Adam',\n",
       " 'Adams',\n",
       " 'Add',\n",
       " 'Additionally',\n",
       " 'Admittedly',\n",
       " 'Ado',\n",
       " 'Adventures',\n",
       " 'Africa',\n",
       " 'African',\n",
       " 'After',\n",
       " 'Again',\n",
       " 'Agatha',\n",
       " 'Age',\n",
       " 'Agent',\n",
       " 'Agnes',\n",
       " 'Ah',\n",
       " 'Aidan',\n",
       " 'Aiello',\n",
       " 'Air',\n",
       " 'Aires',\n",
       " 'Akira',\n",
       " 'Akkad',\n",
       " 'Al',\n",
       " 'Alan',\n",
       " 'Alas',\n",
       " 'Albert',\n",
       " 'Alec',\n",
       " 'Alex',\n",
       " 'Alexander',\n",
       " 'Alfred',\n",
       " 'Alice',\n",
       " 'Alicia',\n",
       " 'Alien',\n",
       " 'Aliens',\n",
       " 'All',\n",
       " 'All-American',\n",
       " 'Allen',\n",
       " 'Alley',\n",
       " 'Allgood',\n",
       " 'Allie',\n",
       " 'Allied',\n",
       " 'Allies',\n",
       " 'Allison',\n",
       " 'Almost',\n",
       " 'Alone',\n",
       " 'Along',\n",
       " 'Also',\n",
       " 'Although',\n",
       " 'Always',\n",
       " 'Am',\n",
       " 'Amazing',\n",
       " 'Amazon',\n",
       " 'Amazon.com',\n",
       " 'Amelia',\n",
       " 'America',\n",
       " 'American',\n",
       " 'Americans',\n",
       " 'Among',\n",
       " 'An',\n",
       " 'Anaconda',\n",
       " 'Anacondas',\n",
       " 'Anchor',\n",
       " 'And',\n",
       " 'Andie',\n",
       " 'Andrew',\n",
       " 'Andrews',\n",
       " 'Andy',\n",
       " 'Angel',\n",
       " 'Angeles',\n",
       " 'Angels',\n",
       " 'Anguish',\n",
       " 'Animagic',\n",
       " 'Animal',\n",
       " 'Animals',\n",
       " 'Ann',\n",
       " 'Anna',\n",
       " 'Annaud',\n",
       " 'Anne',\n",
       " 'Annelle',\n",
       " 'Annis',\n",
       " 'Anniversary',\n",
       " 'Another',\n",
       " 'Anthony',\n",
       " 'Anti-Semitic',\n",
       " 'Antietam',\n",
       " 'Anton',\n",
       " 'Antonio',\n",
       " 'Anxiety',\n",
       " 'Any',\n",
       " 'Anyone',\n",
       " 'Anything',\n",
       " 'Anyway',\n",
       " 'Anyways',\n",
       " 'Apart',\n",
       " 'Apollo',\n",
       " 'Apostles',\n",
       " 'Apparently',\n",
       " 'April',\n",
       " 'Arab',\n",
       " 'Arabic',\n",
       " 'Arabs',\n",
       " 'Aramaic',\n",
       " 'Arc',\n",
       " 'Are',\n",
       " 'Argentina',\n",
       " 'Argento',\n",
       " 'Arizona',\n",
       " 'Arkin',\n",
       " 'Army',\n",
       " 'Arnold',\n",
       " 'Arrow',\n",
       " 'Art',\n",
       " 'Arthur',\n",
       " 'As',\n",
       " 'Ashley',\n",
       " 'Asia',\n",
       " 'Asian',\n",
       " 'Aside',\n",
       " 'Astaire',\n",
       " 'At',\n",
       " 'Atkins',\n",
       " 'Atlantic',\n",
       " 'Attorney',\n",
       " 'Audio',\n",
       " 'Aufschnaiter',\n",
       " 'Austen',\n",
       " 'Austin',\n",
       " 'Australian',\n",
       " 'Austria',\n",
       " 'Austrian',\n",
       " 'Avengers',\n",
       " 'Avenue',\n",
       " 'Avoid',\n",
       " 'Award',\n",
       " 'Awards',\n",
       " 'Awesome',\n",
       " 'Aykroyd',\n",
       " 'B',\n",
       " 'B+',\n",
       " 'B-movie',\n",
       " 'BAD',\n",
       " 'BBC',\n",
       " 'BD',\n",
       " 'BD-Live',\n",
       " 'BE',\n",
       " 'BECAUSE',\n",
       " 'BEFORE',\n",
       " 'BEST',\n",
       " 'BETTER',\n",
       " 'BIBLE',\n",
       " 'BIG',\n",
       " 'BLACK',\n",
       " 'BLU-RAY',\n",
       " 'BOTH',\n",
       " 'BRASCO',\n",
       " 'BTW',\n",
       " 'BUT',\n",
       " 'BUY',\n",
       " 'BY',\n",
       " 'Baby',\n",
       " 'Back',\n",
       " 'Bad',\n",
       " 'Baker',\n",
       " 'Bakshi',\n",
       " 'Balaban',\n",
       " 'Ball',\n",
       " 'Banderas',\n",
       " 'Bandits',\n",
       " 'Barbara',\n",
       " 'Bard',\n",
       " 'Barney',\n",
       " 'Baron',\n",
       " 'Barry',\n",
       " 'Barrymore',\n",
       " 'Barton',\n",
       " 'Baseball',\n",
       " 'Based',\n",
       " 'Basic',\n",
       " 'Basically',\n",
       " 'Basinger',\n",
       " 'Bass',\n",
       " 'Bates',\n",
       " 'Batman',\n",
       " 'Battle',\n",
       " 'Battra',\n",
       " 'Bava',\n",
       " 'Bay',\n",
       " 'Be',\n",
       " 'Bear',\n",
       " 'Beatles',\n",
       " 'Beatrice',\n",
       " 'Beautiful',\n",
       " 'Beautifully',\n",
       " 'Because',\n",
       " 'Beckett',\n",
       " 'Beckinsale',\n",
       " 'Becky',\n",
       " 'Bedouin',\n",
       " 'Beetlejuice',\n",
       " 'Before',\n",
       " 'Behind',\n",
       " 'Being',\n",
       " 'Believe',\n",
       " 'Bell',\n",
       " 'Bellamy',\n",
       " 'Bellamys',\n",
       " 'Bellucci',\n",
       " 'Ben',\n",
       " 'Benedick',\n",
       " 'Benedict',\n",
       " 'Benicio',\n",
       " 'Benjamin',\n",
       " 'Bennett',\n",
       " 'Benny',\n",
       " 'Berkeley',\n",
       " 'Berlin',\n",
       " 'Bernard',\n",
       " 'Berthold',\n",
       " 'Besides',\n",
       " 'Besson',\n",
       " 'Best',\n",
       " 'Better',\n",
       " 'Betty',\n",
       " 'Between',\n",
       " 'Beverly',\n",
       " 'Beyond',\n",
       " 'Bible',\n",
       " 'Biblical',\n",
       " 'Biblically',\n",
       " 'Big',\n",
       " 'Bill',\n",
       " 'Billie',\n",
       " 'Billy',\n",
       " 'Bird',\n",
       " 'Black',\n",
       " 'Blackman',\n",
       " 'Blade',\n",
       " 'Blanche',\n",
       " 'Blazing',\n",
       " 'Bless',\n",
       " 'Blessed',\n",
       " 'Blockbuster',\n",
       " 'Blood',\n",
       " 'Blu',\n",
       " 'Blu-Ray',\n",
       " 'Blu-ray',\n",
       " 'BluRay',\n",
       " 'Blue',\n",
       " 'Bluray',\n",
       " 'Bo',\n",
       " 'Board',\n",
       " 'Boat',\n",
       " 'Bob',\n",
       " 'Bobby',\n",
       " 'Body',\n",
       " 'Bogart',\n",
       " 'Bogie',\n",
       " 'Bond',\n",
       " 'Bonjour',\n",
       " 'Book',\n",
       " 'Books',\n",
       " 'Boot',\n",
       " 'Borgnine',\n",
       " 'Born',\n",
       " 'Both',\n",
       " 'Bottom',\n",
       " 'Boudreaux',\n",
       " 'Bought',\n",
       " 'Box',\n",
       " 'Boy',\n",
       " 'Boys',\n",
       " 'Brad',\n",
       " 'Brain',\n",
       " 'Branagh',\n",
       " 'Brando',\n",
       " 'Brannagh',\n",
       " 'Brasco',\n",
       " 'Brave',\n",
       " 'Braveheart',\n",
       " 'Bravo',\n",
       " 'Brazil',\n",
       " 'Brent',\n",
       " 'Brian',\n",
       " 'Bride',\n",
       " 'Bridges',\n",
       " 'Bridget',\n",
       " 'Briers',\n",
       " 'Brilliant',\n",
       " 'Brimley',\n",
       " 'Brion',\n",
       " 'Brit',\n",
       " 'Britain',\n",
       " 'British',\n",
       " 'Brits',\n",
       " 'Broadway',\n",
       " 'Brolin',\n",
       " 'Bronte',\n",
       " 'Brooklyn',\n",
       " 'Brooks',\n",
       " 'Brother',\n",
       " 'Brothers',\n",
       " 'Brown',\n",
       " 'Bruce',\n",
       " 'Bruno',\n",
       " 'Buck',\n",
       " 'Buckley',\n",
       " 'Buddhist',\n",
       " 'Buenos',\n",
       " 'Bug',\n",
       " 'Bugs',\n",
       " 'Bumble',\n",
       " 'Burgermeister',\n",
       " 'Burke',\n",
       " 'Burl',\n",
       " 'Burton',\n",
       " 'Busey',\n",
       " 'Bush',\n",
       " 'But',\n",
       " 'Butler',\n",
       " 'Buy',\n",
       " 'By',\n",
       " 'C',\n",
       " \"C'mon\",\n",
       " 'C.',\n",
       " 'CAINE',\n",
       " 'CAN',\n",
       " 'CAR',\n",
       " 'CBS',\n",
       " 'CD',\n",
       " 'CE3K',\n",
       " 'CG',\n",
       " 'CGI',\n",
       " 'CHRIST',\n",
       " 'CHRISTMAS',\n",
       " 'CIA',\n",
       " 'CLASSIC',\n",
       " 'CLOSE',\n",
       " 'COULARDEAU',\n",
       " 'COULD',\n",
       " 'Ca',\n",
       " 'Caesar',\n",
       " 'Cage',\n",
       " 'Caiaphas',\n",
       " 'Caine',\n",
       " 'Caiphas',\n",
       " 'Caleb',\n",
       " 'Calvary',\n",
       " 'Came',\n",
       " 'Cameron',\n",
       " 'Campbell',\n",
       " 'Can',\n",
       " 'Canada',\n",
       " 'Capt',\n",
       " 'Captain',\n",
       " 'Car',\n",
       " 'Carl',\n",
       " 'Carlisle',\n",
       " 'Carlos',\n",
       " 'Carlson',\n",
       " 'Carmen',\n",
       " 'Carol',\n",
       " 'Carson',\n",
       " 'Carter',\n",
       " 'Cary',\n",
       " 'Casablanca',\n",
       " 'Casper',\n",
       " 'Cassius',\n",
       " 'Cast',\n",
       " 'Castle',\n",
       " 'Cat',\n",
       " 'Catherine',\n",
       " 'Catholic',\n",
       " 'Catholicism',\n",
       " 'Catholics',\n",
       " 'Cathy',\n",
       " 'Caveziel',\n",
       " 'Caviezel',\n",
       " 'Cecile',\n",
       " 'Celentano',\n",
       " 'Celtic',\n",
       " 'Century',\n",
       " 'Certainly',\n",
       " 'Chad',\n",
       " 'Chairs',\n",
       " 'Chambers',\n",
       " 'Channel',\n",
       " 'Character',\n",
       " 'Charles',\n",
       " 'Charlie',\n",
       " 'Charlotte',\n",
       " 'Check',\n",
       " 'Chesnick',\n",
       " 'Chicago',\n",
       " 'Chief',\n",
       " 'Child',\n",
       " 'Children',\n",
       " 'China',\n",
       " 'Chinese',\n",
       " 'Chris',\n",
       " 'Christ',\n",
       " 'Christian',\n",
       " 'Christianity',\n",
       " 'Christians',\n",
       " 'Christie',\n",
       " 'Christina',\n",
       " 'Christine',\n",
       " 'Christmas',\n",
       " 'Christmastime',\n",
       " 'Christmastown',\n",
       " 'Christopher',\n",
       " 'Church',\n",
       " 'Churchill',\n",
       " 'Ciaran',\n",
       " 'Cinema',\n",
       " 'Cinematography',\n",
       " 'City',\n",
       " 'Civil',\n",
       " 'Claire',\n",
       " 'Clairee',\n",
       " 'Clancy',\n",
       " 'Clarice',\n",
       " 'Clark',\n",
       " 'Clarke',\n",
       " 'Classic',\n",
       " 'Classics',\n",
       " 'Claude',\n",
       " 'Claudio',\n",
       " 'Claus',\n",
       " 'Clause',\n",
       " 'Clavell',\n",
       " 'Clay',\n",
       " 'Clearly',\n",
       " 'Clinton',\n",
       " 'Cloris',\n",
       " 'Close',\n",
       " 'Club',\n",
       " 'Code',\n",
       " 'Cold',\n",
       " 'Colin',\n",
       " 'Collection',\n",
       " 'Collector',\n",
       " 'Collins',\n",
       " 'Color',\n",
       " 'Colors',\n",
       " 'Columbia',\n",
       " 'Come',\n",
       " 'Comedy',\n",
       " 'Comin',\n",
       " 'Coming',\n",
       " 'Commander',\n",
       " 'Commandments',\n",
       " 'Commentary',\n",
       " 'Commodores',\n",
       " 'Communism',\n",
       " 'Communist',\n",
       " 'Company',\n",
       " 'Compare',\n",
       " 'Compared',\n",
       " 'Complete',\n",
       " 'Condorman',\n",
       " 'Connelly',\n",
       " 'Considering',\n",
       " 'Contact',\n",
       " 'Continental',\n",
       " 'Control',\n",
       " 'Cook',\n",
       " 'Cooke',\n",
       " 'Cool',\n",
       " 'Cooper',\n",
       " 'Cop',\n",
       " 'Corbin',\n",
       " 'Corey',\n",
       " 'Corinne',\n",
       " 'Cornelius',\n",
       " 'Corvino',\n",
       " 'Cosmos',\n",
       " 'Could',\n",
       " 'Count',\n",
       " 'Couple',\n",
       " 'Court',\n",
       " 'Cox',\n",
       " 'Craig',\n",
       " 'Crawford',\n",
       " 'Crawley',\n",
       " 'Creepers',\n",
       " 'Crewson',\n",
       " 'Crimson',\n",
       " 'Criterion',\n",
       " 'Critics',\n",
       " 'Cross',\n",
       " 'Crossing',\n",
       " 'Crucifixion',\n",
       " 'Crystal',\n",
       " 'Cuba',\n",
       " 'Cube',\n",
       " 'Culkin',\n",
       " 'Cusack',\n",
       " 'Cut',\n",
       " 'Cute',\n",
       " 'Cyrene',\n",
       " 'D',\n",
       " \"D'Onofrio\",\n",
       " \"D'Urbervilles\",\n",
       " 'D.',\n",
       " 'DA',\n",
       " 'DAY',\n",
       " 'DD',\n",
       " 'DEA',\n",
       " 'DEMONS',\n",
       " 'DID',\n",
       " 'DIFFERENT',\n",
       " 'DJ',\n",
       " 'DNA',\n",
       " 'DNR',\n",
       " 'DO',\n",
       " 'DOES',\n",
       " 'DONE',\n",
       " 'DONNIE',\n",
       " 'DTS',\n",
       " 'DTS-HD',\n",
       " 'DVD',\n",
       " 'DVD.The',\n",
       " 'DVDs',\n",
       " 'Dad',\n",
       " 'Dalai',\n",
       " 'Dali',\n",
       " 'Dallas',\n",
       " 'Dalton',\n",
       " 'Damme',\n",
       " 'Dan',\n",
       " 'Dance',\n",
       " 'Danes',\n",
       " 'Danger',\n",
       " 'Daniel',\n",
       " 'Daniels',\n",
       " 'Danny',\n",
       " 'Daria',\n",
       " 'Dario',\n",
       " 'Dark',\n",
       " 'Darlington',\n",
       " 'Darrell',\n",
       " 'Darren',\n",
       " 'Darryl',\n",
       " 'Daryl',\n",
       " 'Das',\n",
       " 'David',\n",
       " 'Davies',\n",
       " 'Davis',\n",
       " 'Dawn',\n",
       " 'Day',\n",
       " 'Days',\n",
       " 'De',\n",
       " 'Dead',\n",
       " 'Dean',\n",
       " 'Death',\n",
       " 'Debney',\n",
       " 'Deborah',\n",
       " 'Debra',\n",
       " 'December',\n",
       " 'Dedee',\n",
       " 'DeeDee',\n",
       " 'Deedee',\n",
       " 'Deep',\n",
       " 'Def',\n",
       " 'Defense',\n",
       " 'Definitely',\n",
       " 'Definition',\n",
       " 'Del',\n",
       " 'Delaware',\n",
       " 'Deleted',\n",
       " 'Deluxe',\n",
       " 'Demme',\n",
       " 'Demons',\n",
       " 'Denise',\n",
       " 'Dennis',\n",
       " 'Denying',\n",
       " 'Denzel',\n",
       " 'Depp',\n",
       " 'Depression',\n",
       " 'Dermot',\n",
       " 'Deschanel',\n",
       " 'Desert',\n",
       " 'Despite',\n",
       " 'Detective',\n",
       " 'Detillo',\n",
       " 'Devil',\n",
       " 'Diamond',\n",
       " 'Diana',\n",
       " 'Diary',\n",
       " 'Diaz',\n",
       " 'Dick',\n",
       " 'Dickens',\n",
       " 'Did',\n",
       " 'Die',\n",
       " 'Dien',\n",
       " 'Dies',\n",
       " 'Different',\n",
       " 'Digital',\n",
       " 'Dillon',\n",
       " 'Dina',\n",
       " 'Directed',\n",
       " 'Director',\n",
       " 'Directors',\n",
       " 'Disc',\n",
       " 'Disco',\n",
       " 'Disney',\n",
       " 'Distance',\n",
       " 'Diva',\n",
       " 'Divas',\n",
       " 'Dizzy',\n",
       " 'Dmytryk',\n",
       " 'Do',\n",
       " 'Dobbin',\n",
       " 'Doctor',\n",
       " 'Does',\n",
       " 'Dogberry',\n",
       " 'Dogs',\n",
       " 'Dolby',\n",
       " 'Dolly',\n",
       " 'Dolorous',\n",
       " 'Dolph',\n",
       " 'Dom',\n",
       " 'Don',\n",
       " 'Donald',\n",
       " 'Donna',\n",
       " 'Donnie',\n",
       " 'Donovan',\n",
       " 'Doogie',\n",
       " 'Doris',\n",
       " 'Dorothy',\n",
       " 'Dottie',\n",
       " 'Double',\n",
       " 'Doug',\n",
       " 'Douglas',\n",
       " 'Down',\n",
       " 'Downey',\n",
       " 'Downstairs',\n",
       " 'Downton',\n",
       " 'Doyle',\n",
       " 'Dr',\n",
       " 'Dr.',\n",
       " 'Dracula',\n",
       " 'Drama',\n",
       " 'Dream',\n",
       " 'Dreams',\n",
       " 'Dreyfus',\n",
       " 'Dreyfuss',\n",
       " 'Driver',\n",
       " 'Drum',\n",
       " 'Drummer',\n",
       " 'Duchamp',\n",
       " 'Due',\n",
       " 'Duel',\n",
       " 'Dugan',\n",
       " 'Dukakis',\n",
       " 'Duke',\n",
       " 'Dunne',\n",
       " 'Durante',\n",
       " 'During',\n",
       " 'Dutch',\n",
       " 'Duvall',\n",
       " 'Dylan',\n",
       " 'E',\n",
       " 'E.',\n",
       " 'E.G',\n",
       " 'E.T',\n",
       " 'EDITION',\n",
       " 'ELEMENT',\n",
       " 'ENCOUNTERS',\n",
       " 'END',\n",
       " 'ENJOY',\n",
       " 'ET',\n",
       " 'EVEN',\n",
       " 'EVER',\n",
       " 'EVERY',\n",
       " 'EVERYONE',\n",
       " 'EXCELLENT',\n",
       " 'Each',\n",
       " 'Earth',\n",
       " 'East',\n",
       " 'Easter',\n",
       " 'Eastern',\n",
       " 'Eastland',\n",
       " 'Eatenton',\n",
       " 'Eaton',\n",
       " 'Ebert',\n",
       " 'Eckhart',\n",
       " 'Ed',\n",
       " 'Eddie',\n",
       " 'Edgar',\n",
       " 'Edgeware',\n",
       " 'Edgware',\n",
       " 'Edition',\n",
       " 'Edward',\n",
       " 'Edwardian',\n",
       " 'Edwards',\n",
       " 'Effects',\n",
       " 'Egypt',\n",
       " 'Egyptian',\n",
       " 'Eiji',\n",
       " 'Eileen',\n",
       " 'Either',\n",
       " 'Element',\n",
       " 'Elf',\n",
       " 'Elizabeth',\n",
       " 'Ellen',\n",
       " 'Elmo',\n",
       " 'Elsa',\n",
       " 'Elton',\n",
       " 'Elvis',\n",
       " 'Emily',\n",
       " 'Emma',\n",
       " 'Emmerich',\n",
       " 'Empire',\n",
       " 'Encounters',\n",
       " 'End',\n",
       " 'Enemy',\n",
       " 'England',\n",
       " 'English',\n",
       " 'Enjoy',\n",
       " 'Enjoyed',\n",
       " 'Enough',\n",
       " 'Ensign',\n",
       " 'Enter',\n",
       " 'Entertaining',\n",
       " 'Entertainment',\n",
       " 'Episode',\n",
       " 'Equally',\n",
       " 'Era',\n",
       " 'Eric',\n",
       " 'Ernest',\n",
       " 'Ernie',\n",
       " 'Especially',\n",
       " 'Ethan',\n",
       " 'Ethel',\n",
       " 'Eugene',\n",
       " 'Euro',\n",
       " 'Europe',\n",
       " 'European',\n",
       " 'Eve',\n",
       " 'Even',\n",
       " 'Eventually',\n",
       " 'Ever',\n",
       " 'Everett',\n",
       " 'Every',\n",
       " 'Everybody',\n",
       " 'Everyone',\n",
       " 'Everything',\n",
       " 'Evil',\n",
       " 'Ewan',\n",
       " 'Excellent',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_tf(counts, bow):\n",
    "    desc = []\n",
    "    for word in bow:\n",
    "        desc.append(counts[word])\n",
    "    desc = np.array(desc)\n",
    "    desc[np.where(desc is None)] = 0\n",
    "    if sum(desc) != 0:\n",
    "        des = desc / sum(desc)\n",
    "    else:\n",
    "        des = desc\n",
    "    return des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfs = [to_tf(r, bag_2) for r in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def idf(bow, counts):\n",
    "    idfs = []\n",
    "    total = len(counts)\n",
    "    for word in bow:\n",
    "        idfs.append(total / np.count_nonzero([g[word] for g in counts]))\n",
    "    return np.log10(np.array(idfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(tf, idf):\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idfs = idf(bag_2, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_dist(a, b):\n",
    "    abd = a.dot(b)\n",
    "    mag_a = np.sqrt(np.sum(a ** 2))\n",
    "    mag_b = np.sqrt(np.sum(b ** 2))\n",
    "    return (abd / (mag_a * mag_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf = tfidf(tfs, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = tf_idf\n",
    "x_train = inputs[:7000]\n",
    "x_val = inputs[7000:9000]\n",
    "x_test = inputs[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4508, 4508, 4508, ..., 9208, 9208, 9208], dtype=int64),\n",
       " array([   0,    1,    2, ..., 9997, 9998, 9999], dtype=int64))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(tfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model_out, labels):\n",
    "    \"\"\" Computes the mean accuracy, given predictions and true-labels.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model_out : numpy.ndarray, shape=(N, K)\n",
    "            The predicted class-scores/probabilities\n",
    "        labels : numpy.ndarray, shape=(N,)\n",
    "            The labels for the data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The mean classification accuracy of the N samples.\"\"\"\n",
    "    return np.mean(np.argmax(model_out, axis=1) == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(param, rate):\n",
    "    \"\"\" Performs a gradient-descent update on the parameter.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        param : mygrad.Tensor\n",
    "            The parameter to be updated.\n",
    "        \n",
    "        rate : float\n",
    "            The step size used in the update\"\"\"\n",
    "    param.data -= rate*param.grad\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def he_normal(shape):\n",
    "    \"\"\" Given the desired shape of your array, draws random\n",
    "        values from a scaled-Gaussian distribution.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\"\"\"\n",
    "    N = shape[0]\n",
    "    scale = 1 / np.sqrt(2*N)\n",
    "    return np.random.randn(*shape)*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"save_data.txt\", \"rb\") as f:\n",
    "    reviews, ratings, inputs, bag_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"save_data.txt\", \"wb\") as f:\n",
    "    pickle.dump([reviews, ratings, inputs, bag_2], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02067323,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.00800254,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x, y, xval, yval, step=1, iters=400):\n",
    "    w1 = Tensor(he_normal((x.shape[1], 2000)))\n",
    "    print(w1)\n",
    "    b1 = Tensor(np.zeros(2000, dtype=w1.dtype))\n",
    "    w2 = Tensor(he_normal((2000, 300)))\n",
    "    b2 = Tensor(np.zeros(300, dtype=w2.dtype))\n",
    "    w3 = Tensor(he_normal((300, 5)))\n",
    "    b3 = Tensor(np.zeros(5, dtype=w3.dtype))\n",
    "    params = [b1, w1, b2, w2, b3, w3]\n",
    "\n",
    "    y_cross = np.zeros((x.shape[0], 5))\n",
    "    y_cross[np.arange(x.shape[0]), y] = 1\n",
    "    \n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    t0 = time.time()\n",
    "    for i in range(iters):\n",
    "        if i % 10 == 0:\n",
    "            print(i, time.time() - t0)\n",
    "        o1 = relu(dense(x, w1) + b1)\n",
    "        o2 = relu(dense(o1, w2) + b2)\n",
    "        y_pred = softmax(dense(o2, w3) + b3)\n",
    "        \n",
    "        loss = cross_entropy(y_pred, y_cross)\n",
    "        losses.append(loss.data.item())\n",
    "        loss.backward()\n",
    "\n",
    "        accuracies.append(compute_accuracy(y_pred.data, y))\n",
    "\n",
    "        for param in params:\n",
    "            sgd(param, step)\n",
    "        print(w1)\n",
    "\n",
    "        loss.null_gradients()\n",
    "    \n",
    "    o1 = relu(dense(x, w1) + b1)\n",
    "    o2 = relu(dense(o1, w2) + b2)\n",
    "    train_pred = softmax(dense(o2, w3) + b3)\n",
    "    train_acc = compute_accuracy(train_pred, y)\n",
    "    \n",
    "    o1 = relu(dense(xval, w1) + b1)\n",
    "    o2 = relu(dense(o1, w2) + b2)\n",
    "    val_pred = softmax(dense(o2, w3) + b3)\n",
    "    val_acc = compute_accuracy(val_pred, yval)\n",
    "    print(\"Training accuracy:\", train_acc, \"; Validation accuracy:\", val_acc)\n",
    "    return train_acc, val_acc, accuracies, losses, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "Tensor(\n",
      "[[ 0.20007748  0.19993356  0.19997504  0.19994132  0.2000726 ]\n",
      " [ 0.19990703  0.19986896  0.19990928  0.20016816  0.20014657]\n",
      " [ 0.20009903  0.19983644  0.19993463  0.19996813  0.20016177]\n",
      " ..., \n",
      " [ 0.20004149  0.19998132  0.19993382  0.19996179  0.20008158]\n",
      " [ 0.19999416  0.20001516  0.20004087  0.19997383  0.19997598]\n",
      " [ 0.19990744  0.19995428  0.19985735  0.20011498  0.20016595]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.60973136 -0.        ]\n",
      " [-0.         -0.         -1.60989163 -0.         -0.        ]\n",
      " [-0.         -0.         -1.60976483 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.60962896 -0.        ]\n",
      " [-0.         -0.         -0.         -1.60956878 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -1.60860851]]\n",
      ")\n",
      "Tensor(-11264.527660485668)\n",
      "Tensor(\n",
      "[[ 0.16240377  0.16059909  0.16965832  0.19297455  0.31436427]\n",
      " [ 0.16213566  0.16053631  0.16950455  0.19309104  0.31473244]\n",
      " [ 0.16250318  0.16055295  0.1696224   0.19303622  0.31428525]\n",
      " ..., \n",
      " [ 0.16247187  0.16056774  0.16950916  0.19300939  0.31444185]\n",
      " [ 0.16240555  0.1606429   0.16955649  0.19304956  0.3143455 ]\n",
      " [ 0.16252677  0.16055167  0.16953907  0.19301068  0.31437182]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.64519699 -0.        ]\n",
      " [-0.         -0.         -1.77487552 -0.         -0.        ]\n",
      " [-0.         -0.         -1.7741805  -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.64501644 -0.        ]\n",
      " [-0.         -0.         -0.         -1.64480836 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -1.15717884]]\n",
      ")\n",
      "Tensor(-9636.389570178762)\n",
      "Tensor(\n",
      "[[ 0.12517265  0.12351239  0.13685759  0.17669405  0.43776331]\n",
      " [ 0.12502219  0.1235061   0.13675394  0.17680789  0.43790988]\n",
      " [ 0.12525353  0.12351097  0.1368128   0.17673303  0.43768966]\n",
      " ..., \n",
      " [ 0.12523728  0.123485    0.13674514  0.17666303  0.43786954]\n",
      " [ 0.12514571  0.12353801  0.13673144  0.17672689  0.43785794]\n",
      " [ 0.12528485  0.12350161  0.13678564  0.17670849  0.43771941]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.73333555 -0.        ]\n",
      " [-0.         -0.         -1.98957202 -0.         -0.        ]\n",
      " [-0.         -0.         -1.9891417  -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.73351114 -0.        ]\n",
      " [-0.         -0.         -0.         -1.73314971 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.82617719]]\n",
      ")\n",
      "Tensor(-8664.527220095608)\n",
      "Tensor(\n",
      "[[ 0.09859313  0.09687906  0.11307467  0.16370006  0.52775308]\n",
      " [ 0.0984785   0.09686772  0.11301665  0.16378789  0.52784925]\n",
      " [ 0.09865842  0.09687584  0.11302975  0.16374522  0.52769077]\n",
      " ..., \n",
      " [ 0.09865642  0.09683836  0.112972    0.1636677   0.52786552]\n",
      " [ 0.09857178  0.09689668  0.11296185  0.16371846  0.52785122]\n",
      " [ 0.09867117  0.09688863  0.11302163  0.16372217  0.52769641]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.80971942 -0.        ]\n",
      " [-0.         -0.         -2.18022013 -0.         -0.        ]\n",
      " [-0.         -0.         -2.18010422 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.80991714 -0.        ]\n",
      " [-0.         -0.         -0.         -1.80960702 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.63923414]]\n",
      ")\n",
      "Tensor(-8261.202513414999)\n",
      "Tensor(\n",
      "[[ 0.08272053  0.08071407  0.09969313  0.16022179  0.57665047]\n",
      " [ 0.0826246   0.08068244  0.09962279  0.16031022  0.57675995]\n",
      " [ 0.08276586  0.08070666  0.09966639  0.16028768  0.5765734 ]\n",
      " ..., \n",
      " [ 0.08276418  0.08066517  0.09961615  0.16022551  0.576729  ]\n",
      " [ 0.08269437  0.0807308   0.09960816  0.16025878  0.57670788]\n",
      " [ 0.08277547  0.08072674  0.09966695  0.16025077  0.57658007]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.83119624 -0.        ]\n",
      " [-0.         -0.         -2.3063643  -0.         -0.        ]\n",
      " [-0.         -0.         -2.30592674 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.83117304 -0.        ]\n",
      " [-0.         -0.         -0.         -1.8309654  -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.55064107]]\n",
      ")\n",
      "Tensor(-8116.390068636206)\n",
      "Tensor(\n",
      "[[ 0.07379793  0.07147181  0.09345687  0.16420298  0.5970704 ]\n",
      " [ 0.07371713  0.07143095  0.0933628   0.16426192  0.5972272 ]\n",
      " [ 0.07383489  0.07146543  0.09342989  0.16426955  0.59700024]\n",
      " ..., \n",
      " [ 0.07383527  0.07142637  0.09338195  0.16421926  0.59713714]\n",
      " [ 0.07377493  0.07148928  0.09337715  0.1642543   0.59710434]\n",
      " [ 0.07384213  0.07148501  0.09342998  0.1642302   0.59701268]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.80665192 -0.        ]\n",
      " [-0.         -0.         -2.37126229 -0.         -0.        ]\n",
      " [-0.         -0.         -2.37054399 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.80655278 -0.        ]\n",
      " [-0.         -0.         -0.         -1.80633947 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.51581692]]\n",
      ")\n",
      "Tensor(-8055.3806708297925)\n",
      "Tensor(\n",
      "[[ 0.06834071  0.06569688  0.09064229  0.17100798  0.60431213]\n",
      " [ 0.06825144  0.0656497   0.09053909  0.17106903  0.60449074]\n",
      " [ 0.06836506  0.06569646  0.09061329  0.17107885  0.60424634]\n",
      " ..., \n",
      " [ 0.06837034  0.06565699  0.09056539  0.17104149  0.60436579]\n",
      " [ 0.06831868  0.06571634  0.0905647   0.17107087  0.6043294 ]\n",
      " [ 0.06837629  0.06570852  0.0906131   0.17103701  0.60426507]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.76604503 -0.        ]\n",
      " [-0.         -0.         -2.40197358 -0.         -0.        ]\n",
      " [-0.         -0.         -2.40115436 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.7658491  -0.        ]\n",
      " [-0.         -0.         -0.         -1.76567734 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.50374231]]\n",
      ")\n",
      "Tensor(-8021.444542758569)\n",
      "Tensor(\n",
      "[[ 0.06454844  0.06161484  0.08930059  0.17773112  0.60680501]\n",
      " [ 0.06444723  0.06155881  0.08919466  0.17775629  0.607043  ]\n",
      " [ 0.06456362  0.06162657  0.08924675  0.17779434  0.60676873]\n",
      " ..., \n",
      " [ 0.06457387  0.06157608  0.08922666  0.1777705   0.60685289]\n",
      " [ 0.06452699  0.06163573  0.08922902  0.17779399  0.60681428]\n",
      " [ 0.06457558  0.0616256   0.08927313  0.17775478  0.6067709 ]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.72748344 -0.        ]\n",
      " [-0.         -0.         -2.41693407 -0.         -0.        ]\n",
      " [-0.         -0.         -2.41635033 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.72726187 -0.        ]\n",
      " [-0.         -0.         -0.         -1.72712974 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49960399]]\n",
      ")\n",
      "Tensor(-8000.5353002385455)\n",
      "Tensor(\n",
      "[[ 0.06169615  0.05851605  0.08866723  0.18327869  0.60784188]\n",
      " [ 0.06158926  0.05845341  0.08854909  0.18329046  0.60811778]\n",
      " [ 0.06170608  0.05853036  0.0886107   0.18334106  0.60781181]\n",
      " ..., \n",
      " [ 0.0617203   0.05847928  0.08859082  0.18333796  0.60787164]\n",
      " [ 0.06167517  0.05853846  0.08859811  0.18337437  0.60781389]\n",
      " [ 0.06171743  0.05852601  0.08863721  0.18331506  0.6078043 ]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.69674739 -0.        ]\n",
      " [-0.         -0.         -2.42419821 -0.         -0.        ]\n",
      " [-0.         -0.         -2.42350271 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.69642407 -0.        ]\n",
      " [-0.         -0.         -0.         -1.69622549 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49790233]]\n",
      ")\n",
      "Tensor(-7987.457874303313)\n",
      "Tensor(\n",
      "[[ 0.05949297  0.05609275  0.08843574  0.18758893  0.60838961]\n",
      " [ 0.05938438  0.05602528  0.08830702  0.18757136  0.60871197]\n",
      " [ 0.05950111  0.05610737  0.08837621  0.18761685  0.60839846]\n",
      " ..., \n",
      " [ 0.05951406  0.05605807  0.08836797  0.18765254  0.60840736]\n",
      " [ 0.0594759   0.05611648  0.08837049  0.18768039  0.60835673]\n",
      " [ 0.05951166  0.0561009   0.08840786  0.18762157  0.60835802]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.67350227 -0.        ]\n",
      " [-0.         -0.         -2.4269357  -0.         -0.        ]\n",
      " [-0.         -0.         -2.42615249 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.67316322 -0.        ]\n",
      " [-0.         -0.         -0.         -1.67301479 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49699172]]\n",
      ")\n",
      "Tensor(-7979.244997021818)\n",
      "10 193.1008324623108\n",
      "Tensor(\n",
      "[[ 0.05777525  0.0541776   0.0884837   0.19077492  0.60878852]\n",
      " [ 0.05766191  0.05410425  0.08834261  0.19077497  0.60911627]\n",
      " [ 0.05778437  0.05418989  0.08842336  0.19078798  0.6088144 ]\n",
      " ..., \n",
      " [ 0.05779773  0.05414452  0.08840916  0.19082089  0.60882769]\n",
      " [ 0.05776021  0.05420016  0.08841858  0.19090068  0.60872038]\n",
      " [ 0.05779298  0.05418343  0.0884512   0.19081762  0.60875478]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.65666096 -0.        ]\n",
      " [-0.         -0.         -2.42653278 -0.         -0.        ]\n",
      " [-0.         -0.         -2.42561912 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.65642004 -0.        ]\n",
      " [-0.         -0.         -0.         -1.65600201 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49633976]]\n",
      ")\n",
      "Tensor(-7973.994861030116)\n",
      "Tensor(\n",
      "[[ 0.05642789  0.05265229  0.08870724  0.19316168  0.6090509 ]\n",
      " [ 0.0563073   0.05257697  0.08856724  0.19313231  0.60941618]\n",
      " [ 0.0564368   0.05266232  0.08865196  0.19317314  0.60907578]\n",
      " ..., \n",
      " [ 0.05644902  0.05262288  0.08863358  0.19319998  0.60909454]\n",
      " [ 0.0564127   0.05267664  0.08865831  0.19328751  0.60896484]\n",
      " [ 0.05644299  0.05265756  0.08867777  0.19320003  0.60902166]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.64422774 -0.        ]\n",
      " [-0.         -0.         -2.42399328 -0.         -0.        ]\n",
      " [-0.         -0.         -2.42303719 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.64402945 -0.        ]\n",
      " [-0.         -0.         -0.         -1.64357652 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49590145]]\n",
      ")\n",
      "Tensor(-7970.567777861492)\n",
      "Tensor(\n",
      "[[ 0.05535598  0.05142978  0.08906814  0.19485796  0.60928814]\n",
      " [ 0.05523576  0.05134898  0.08891069  0.19482647  0.60967809]\n",
      " [ 0.05536581  0.05143821  0.08901432  0.19485964  0.60932203]\n",
      " ..., \n",
      " [ 0.05537633  0.05140298  0.08899806  0.19490371  0.60931892]\n",
      " [ 0.05534482  0.0514548   0.08901574  0.19499963  0.60918501]\n",
      " [ 0.05536895  0.05143378  0.08903954  0.19491622  0.60924151]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.63548439 -0.        ]\n",
      " [-0.         -0.         -2.42012286 -0.         -0.        ]\n",
      " [-0.         -0.         -2.41895807 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.63524966 -0.        ]\n",
      " [-0.         -0.         -0.         -1.63475763 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49554052]]\n",
      ")\n",
      "Tensor(-7968.285969445079)\n",
      "Tensor(\n",
      "[[ 0.05451341  0.05044041  0.08947948  0.19610572  0.60946098]\n",
      " [ 0.05438759  0.05035611  0.08932087  0.19605277  0.60988265]\n",
      " [ 0.05452262  0.05044815  0.08942666  0.19610577  0.6094968 ]\n",
      " ..., \n",
      " [ 0.05453504  0.05041443  0.08940989  0.1961526   0.60948804]\n",
      " [ 0.05450174  0.05046652  0.08943582  0.19624828  0.60934764]\n",
      " [ 0.05452553  0.05044331  0.08944731  0.19615678  0.60942707]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.62910139 -0.        ]\n",
      " [-0.         -0.         -2.41552011 -0.         -0.        ]\n",
      " [-0.         -0.         -2.41433647 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.62886238 -0.        ]\n",
      " [-0.         -0.         -0.         -1.62837471 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.495236  ]]\n",
      ")\n",
      "Tensor(-7966.719422982323)\n",
      "Tensor(\n",
      "[[ 0.05383846  0.04963725  0.08992892  0.1969697   0.60962566]\n",
      " [ 0.0537099   0.04954775  0.08976278  0.1969372   0.61004237]\n",
      " [ 0.05384741  0.04964466  0.08987844  0.19697392  0.60965557]\n",
      " ..., \n",
      " [ 0.053861    0.0496121   0.08986046  0.19701587  0.60965057]\n",
      " [ 0.0538288   0.04966338  0.08988579  0.19713245  0.60948957]\n",
      " [ 0.0538493   0.04963903  0.08989432  0.19702754  0.60958981]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.62470536 -0.        ]\n",
      " [-0.         -0.         -2.41058482 -0.         -0.        ]\n",
      " [-0.         -0.         -2.40929723 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.62447097 -0.        ]\n",
      " [-0.         -0.         -0.         -1.62387945 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49496899]]\n",
      ")\n",
      "Tensor(-7965.625945910118)\n",
      "Tensor(\n",
      "[[ 0.05329973  0.04897829  0.09037882  0.19764105  0.60970211]\n",
      " [ 0.05316716  0.04888643  0.09020551  0.1975871   0.6101538 ]\n",
      " [ 0.05330885  0.04898549  0.09032856  0.19764571  0.60973138]\n",
      " ..., \n",
      " [ 0.05332291  0.04895399  0.09031136  0.19769017  0.60972157]\n",
      " [ 0.05329165  0.04900534  0.09033807  0.19780645  0.60955848]\n",
      " [ 0.05330914  0.04897931  0.09034204  0.19770085  0.60966867]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.62130278 -0.        ]\n",
      " [-0.         -0.         -2.40566472 -0.         -0.        ]\n",
      " [-0.         -0.         -2.40430156 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.62105425 -0.        ]\n",
      " [-0.         -0.         -0.         -1.62046625 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49483964]]\n",
      ")\n",
      "Tensor(-7964.845911851724)\n",
      "Tensor(\n",
      "[[ 0.05286509  0.04843431  0.09080279  0.19809346  0.60980435]\n",
      " [ 0.05272845  0.04833886  0.09062546  0.19804274  0.6102645 ]\n",
      " [ 0.05287441  0.04844124  0.09075228  0.19809619  0.60983588]\n",
      " ..., \n",
      " [ 0.05288891  0.04841124  0.09073525  0.19814312  0.60982147]\n",
      " [ 0.0528591   0.04846146  0.09076342  0.19826866  0.60964736]\n",
      " [ 0.05287311  0.04843455  0.09076352  0.19815586  0.60977297]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.61901632 -0.        ]\n",
      " [-0.         -0.         -2.40102013 -0.         -0.        ]\n",
      " [-0.         -0.         -2.39962173 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.61876568 -0.        ]\n",
      " [-0.         -0.         -0.         -1.6181323  -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49466858]]\n",
      ")\n",
      "Tensor(-7964.282031452108)\n",
      "Tensor(\n",
      "[[ 0.0525126   0.04798243  0.09121102  0.19845758  0.60983637]\n",
      " [ 0.05237288  0.04788512  0.09102452  0.19838361  0.61033387]\n",
      " [ 0.05252144  0.04798961  0.09116051  0.19845988  0.60986856]\n",
      " ..., \n",
      " [ 0.052537    0.04796064  0.09114316  0.19850595  0.60985326]\n",
      " [ 0.0525076   0.04801108  0.09117188  0.19863033  0.60967912]\n",
      " [ 0.05251927  0.04798166  0.09116849  0.19852383  0.60980674]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.61717989 -0.        ]\n",
      " [-0.         -0.         -2.39662631 -0.         -0.        ]\n",
      " [-0.         -0.         -2.39513347 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.61693623 -0.        ]\n",
      " [-0.         -0.         -0.         -1.61630984 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49461319]]\n",
      ")\n",
      "Tensor(-7963.865797635675)\n",
      "Tensor(\n",
      "[[ 0.05222678  0.04760517  0.09156916  0.19866916  0.60992973]\n",
      " [ 0.05208335  0.04750342  0.09137854  0.19861943  0.61041527]\n",
      " [ 0.05223505  0.04761196  0.09151939  0.19867014  0.60996346]\n",
      " ..., \n",
      " [ 0.05225142  0.04758462  0.0915014   0.19871581  0.60994674]\n",
      " [ 0.05222238  0.04763445  0.09153269  0.19885595  0.60975453]\n",
      " [ 0.05223127  0.04760375  0.09152466  0.19873295  0.60990737]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.61611436 -0.        ]\n",
      " [-0.         -0.         -2.39274463 -0.         -0.        ]\n",
      " [-0.         -0.         -2.39120442 -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.61587955 -0.        ]\n",
      " [-0.         -0.         -0.         -1.61517458 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49444818]]\n",
      ")\n",
      "Tensor(-7963.555764295723)\n",
      "Tensor(\n",
      "[[ 0.05199355  0.04728909  0.09190617  0.19889204  0.60991914]\n",
      " [ 0.0518468   0.04718485  0.09170862  0.19882187  0.61043787]\n",
      " [ 0.05200171  0.04729574  0.09185476  0.19889062  0.60995717]\n",
      " ..., \n",
      " [ 0.05201868  0.04726937  0.09183874  0.19893861  0.60993461]\n",
      " [ 0.0519902   0.04731919  0.09187077  0.19908085  0.60973898]\n",
      " [ 0.05199645  0.04728661  0.09185932  0.198955    0.60990262]]\n",
      ") [[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Tensor(\n",
      "[[-0.         -0.         -0.         -1.61499313 -0.        ]\n",
      " [-0.         -0.         -2.38913891 -0.         -0.        ]\n",
      " [-0.         -0.         -2.3875466  -0.         -0.        ]\n",
      " ..., \n",
      " [-0.         -0.         -0.         -1.61475901 -0.        ]\n",
      " [-0.         -0.         -0.         -1.61404423 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.49445597]]\n",
      ")\n",
      "Tensor(-7963.318000252806)\n",
      "Training accuracy: 0.61 ; Validation accuracy: 0.6005\n"
     ]
    }
   ],
   "source": [
    "ta, va, acc, los, pms = train(x_train, y_train, x_val, y_val, iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38514285714285712,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999,\n",
       " 0.60999999999999999]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
