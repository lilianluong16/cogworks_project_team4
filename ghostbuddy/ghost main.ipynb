{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from nltk.corpus import words\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"english_words.txt\") as word_file:\n",
    "    english_words = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "def is_english_word(word):\n",
    "    return word.lower() in english_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip(pairs):\n",
    "    \"\"\"Splits list of pairs (tuples) into separate lists.\n",
    "    \n",
    "    Example: pairs = [(\"a\", 1), (\"b\", 2)] --> [\"a\", \"b\"] and [1, 2]\n",
    "    \n",
    "    This should look familiar from our review back at the beginning of week 1\n",
    "    :)\n",
    "    \"\"\"\n",
    "    return tuple(zip(*pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def normalize(counter):\n",
    "    \"\"\" Convert counter to a list of (letter, frequency) pairs, sorted in descending order of frequency.\n",
    "    \n",
    "        Parameters\n",
    "        -----------\n",
    "        counter: A Counter-instance\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A list of tuples - (letter, frequency) pairs. \n",
    "        \n",
    "        For example, if counter had the counts:\n",
    "        \n",
    "            {'a': 1, 'b': 3}\n",
    "        \n",
    "        `normalize(counter)` will return:\n",
    "        \n",
    "            [('b', 0.75), ('a', 0.25)]\n",
    "    \"\"\"\n",
    "    total = sum(counter.values())\n",
    "    return [(char, cnt/total) for char, cnt in counter.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "def train_lm(text, n):\n",
    "    \"\"\" Train character-based n-gram language model.\n",
    "        \n",
    "        This will learn: given a sequence of n-1 characters, what the probability\n",
    "        distribution is for the n-th character in the sequence.\n",
    "        \n",
    "        For example if we train on the text:\n",
    "            text = \"cacao\"\n",
    "        \n",
    "        Using a n-gram size of n=3, then the following dict would be returned:\n",
    "        \n",
    "            {'ac': [('a', 1.0)],\n",
    "             'ca': [('c', 0.5), ('o', 0.5)],\n",
    "             '~c': [('a', 1.0)],\n",
    "             '~~': [('c', 1.0)]}\n",
    "\n",
    "        Tildas (\"~\") are used for padding the history when necessary, so that it's \n",
    "        possible to estimate the probability of a seeing a character when there \n",
    "        aren't (n - 1) previous characters of history available.\n",
    "        \n",
    "        So, according to this text we trained on, if you see the sequence 'ac',\n",
    "        our model predicts that the next character should be 'a' 100% of the time.\n",
    "        \n",
    "       For generatiing the padding, recall that Python allows you to generate \n",
    "        repeated sequences easily: \n",
    "           `\"p\" * 4` returns `\"pppp\"`\n",
    "           \n",
    "        Parameters\n",
    "        -----------\n",
    "        text: str \n",
    "            A string (doesn't need to be lowercased).\n",
    "        n: int\n",
    "            The length of n-gram to analyze.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        A dict that maps histories (strings of length (n-1)) to lists of (char, prob) \n",
    "        pairs, where prob is the probability (i.e frequency) of char appearing after \n",
    "        that specific history. For example, if\n",
    "\n",
    "    \"\"\"\n",
    "    li = word_tokenize(text)\n",
    "\n",
    "    \n",
    "    \n",
    "    text = ' '.join(list(itertools.filterfalse(lambda x: len(x) % 2 ==0, li)))\n",
    "    \n",
    "    #for el in li:\n",
    "        #if len(el) % 2 !=0:\n",
    "            #li.remove(el)\n",
    "            \n",
    "    #text = ''.join(li)\n",
    "    \n",
    "    raw_lm = defaultdict(Counter)\n",
    "    history = \"~\" * (n - 1)\n",
    "    \n",
    "    # count number of times characters appear following different histories\n",
    "    for x in text:\n",
    "        raw_lm[history][x] += 1\n",
    "        history = history[1:] + x\n",
    "    \n",
    "    # create final dictionary by normalizing\n",
    "\n",
    "    \n",
    "    the_list = list(raw_lm.values())\n",
    "    key_list = list(raw_lm.keys())\n",
    "    \n",
    " \n",
    "    \n",
    "    for i in range(len(the_list)):\n",
    "        \n",
    "        if the_list[i].get(' ') is not None:\n",
    "            del the_list[i][' ']\n",
    "\n",
    "    final_lm = {}\n",
    "    for key, val in zip(key_list,the_list):\n",
    "        final_lm[key] = val\n",
    "        \n",
    "\n",
    "    lm = { history : normalize(counter) for history, counter in final_lm.items() }\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_letter(lm, history):\n",
    "    \"\"\" Randomly picks letter according to probability distribution associated with \n",
    "        the specified history.\n",
    "    \n",
    "        Note: returns dummy character \"~\" if history not found in model.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        lm: Dict[str, Tuple[str, float]] \n",
    "            The n-gram language model. I.e. the dictionary: history -> (char, freq)\n",
    "        \n",
    "        history: str\n",
    "            A string of length (n-1) to use as context/history for generating \n",
    "            the next character.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The predicted character. '~' if history is not in language model.\n",
    "    \"\"\"\n",
    "    if not history in lm:\n",
    "        return \"~\"\n",
    "    letters, probs = unzip(lm[history])\n",
    "    i = np.random.choice(letters, p=probs)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(lm, n, nletters=100):\n",
    "    \"\"\" Randomly generates nletters of text with n-gram language model lm.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        lm: Dict[str, Tuple[str, float]] \n",
    "            The n-gram language model. I.e. the dictionary: history -> (char, freq)\n",
    "        n: int\n",
    "            Order of n-gram model.\n",
    "        nletters: int\n",
    "            Number of letters to randomly generate.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Model-generated text.\n",
    "    \"\"\"\n",
    "    history = \"~\" * (n - 1)\n",
    "    text = []\n",
    "    for i in range(nletters):\n",
    "        \n",
    "        if i <10:\n",
    "            c = generate_letter(lm, history)\n",
    "            print(c)\n",
    "            text.append(c)\n",
    "            history = history[1:] + c\n",
    "            print(history)\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            c = generate_letter(lm, history)\n",
    "            text.append(c)\n",
    "            history = history[1:] + c  \n",
    "        \n",
    "        \n",
    "    return \"\".join(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1044754 character(s)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "path_to_merged = \"english_words.txt\"\n",
    "with open(path_to_merged , \"r\") as f:\n",
    "    twentyk  = f.read()\n",
    "print(str(len(twentyk )) + \" character(s)\")\n",
    "chars = set(twentyk)\n",
    "print(\"~\" in chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "elapsed = 1.2819085121154785s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "lm3 = train_lm(twentyk, 3)\n",
    "t1 = time.time()\n",
    "print(type(lm3))\n",
    "print(\"elapsed = \" + str(t1 - t0) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed = 1.7542412281036377s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "lm5 = train_lm(twentyk, 5)\n",
    "t1 = time.time()\n",
    "print(\"elapsed = \" + str(t1 - t0) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed = 3.6996219158172607s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "lm7 = train_lm(twentyk, 7)\n",
    "t1 = time.time()\n",
    "print(\"elapsed = \" + str(t1 - t0) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuv\n"
     ]
    }
   ],
   "source": [
    "import random, string\n",
    "\n",
    "first_letter = random.choice(string.ascii_letters[:22])\n",
    "\n",
    "print(string.ascii_letters[:22])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sec = input(\"Please input the next letter: \")\n",
    "\n",
    "string = first_letter + sec\n",
    "\n",
    "print(string)\n",
    "\n",
    "history = string\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "\n",
    "    \n",
    "    if i == 0:\n",
    "        \n",
    "        letter = generate_letter(lm3,history)\n",
    "        history = history[1:] + letter\n",
    "        \n",
    "        string +=letter\n",
    "        \n",
    "        print(string)\n",
    "        \n",
    "        if is_english_word(string) and len(string) > 3:\n",
    "            print(\"The computer loses!\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "    if i %2 ==1:\n",
    "        \n",
    "        letter = input(\"Please input the next letter: \")\n",
    "        \n",
    "        string += letter\n",
    "        \n",
    "        print(string)\n",
    "\n",
    "        if is_english_word(string) and len(string) > 3:\n",
    "            print(\"You lose!\")\n",
    "            break\n",
    "            \n",
    "       \n",
    "    if i >= 2 and i%2 ==0 and i < 4:\n",
    "        \n",
    "        letter = generate_letter(lm5,string[-4:])\n",
    "        history = history[1:] + letter\n",
    "        \n",
    "        string +=letter\n",
    "        \n",
    "        print(string)\n",
    "        \n",
    "        if is_english_word(string) and len(string) > 3:\n",
    "            print(\"The computer loses!\")\n",
    "            break\n",
    "            \n",
    "    if i >= 2 and i%2 ==0 and i >= 4:\n",
    "        \n",
    "        letter = generate_letter(lm7,string[-6:])\n",
    "        history = history[1:] + letter\n",
    "        \n",
    "        string +=letter\n",
    "        \n",
    "        print(string)\n",
    "        \n",
    "        if is_english_word(string) and len(string) > 3:\n",
    "            print(\"The computer loses!\")\n",
    "            break\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
