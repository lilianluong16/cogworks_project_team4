{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, pickle\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import os\n",
    "from os import path, makedirs\n",
    "import news_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single(tokens, by_type=False):\n",
    "    \"\"\"\n",
    "    Gets the named entities in the list of tokens\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tokens:\n",
    "        a tokenized document containing named entities\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of strings:\n",
    "        each string is the name of an entity\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    pos = nltk.pos_tag(tokens) # label parts of speech\n",
    "    named_entities = nltk.ne_chunk(pos, binary=not by_type) # identify named entities\n",
    "    for i in range(0, len(named_entities)):\n",
    "        ents = named_entities.pop()\n",
    "        if getattr(ents, 'label', None) != None and ents.label() == \"NE\": \n",
    "            entities.append(([ne for ne in ents]))\n",
    "    extracted = np.array(entities)\n",
    "    if extracted.ndim == 3:\n",
    "        final = extracted[:,:,0].tolist()\n",
    "    else:\n",
    "        final = []\n",
    "        for entity in extracted:\n",
    "            final.append(np.array(entity)[:,0].tolist())\n",
    "    r = []\n",
    "    for entity in final:\n",
    "        entity = \" \".join(entity)\n",
    "        r.append(entity)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relate_entities(extracted):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        Counter:\n",
    "            \n",
    "    \"\"\"\n",
    "    occurences = Counter(extracted)\n",
    "    return occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_db(extracted=None):\n",
    "    \"\"\"\n",
    "    Creates a database of named entities occuring the supplied article\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    extracted (list of strings):\n",
    "        the extracted named entities from a single documetn\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dictionary (string --> Counter):\n",
    "        database relating the named entities to \n",
    "    \"\"\"\n",
    "    global db\n",
    "    db = {}\n",
    "    if extracted is not None:\n",
    "        for entity in extracted:\n",
    "            db[entity] = Counter(extracted)\n",
    "            del db[entity][entity]\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_single(extracted):\n",
    "    for entity in extracted:\n",
    "        if entity in db:\n",
    "            db[entity].update(Counter(extracted))\n",
    "        else:\n",
    "            db[entity] = Counter(extracted)\n",
    "        del db[entity][entity]\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATABASE_FR = \"data/entities_database.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_database(filepath=DATABASE_FR):\n",
    "    \"\"\"\n",
    "    Creates a new text file and folder in the filepath; uses \n",
    "\n",
    "    If creating additional filepaths, specify it in the filepath variable \n",
    "    in all functions with the filepath kwarg\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(str.partition(filepath, \"/\")[0])\n",
    "        with open(filepath, \"w+\"):\n",
    "            pass\n",
    "\n",
    "new_database(filepath=DATABASE_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_database(filepath=DATABASE_FR):\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(db, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_database(filepath=DATABASE_FR):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        db = pickle.load(f)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_database(filepath=DATABASE_FR):\n",
    "    db = {}\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(db, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_related(entity):\n",
    "    return db[entity].most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(docs):\n",
    "    for doc in docs:\n",
    "        update_single(extract_entities(doc))\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading: http://feeds.reuters.com/~r/Reuters/worldNews/~3/fwIVm9YOgzo/us-cyber-france-facebook-spies-exclusive-idUSKBN1AC0EI\n",
      "downloading: http://feeds.reuters.com/~r/Reuters/worldNews/~3/5S-3l10j6wQ/us-usa-trump-russia-sanctions-idUSKBN1AC1U8\n",
      "downloading: http://feeds.reuters.com/~r/Reuters/worldNews/~3/_H47_5h6Up4/us-britain-baby-idUSKBN1AC1C1\n",
      "downloading: http://feeds.reuters.com/~r/Reuters/worldNews/~3/MVgH3StW3FI/us-israel-palestinians-idUSKBN1AC0UF\n",
      "downloading: http://feeds.reuters.com/~r/Reuters/worldNews/~3/Rfs2YUfZuX8/us-usa-northkorea-army-idUSKBN1AC2V3\n",
      "downloading: http://feeds.reuters.com/~r/Reuters/worldNews/~3/BkvXqd7sJk4/us-mideast-crisis-syria-un-idUSKBN1AC315\n",
      "downloading: http://feeds.reuters.com/~r/Reuters/worldNews/~3/9gthxn-Unx4/us-nigeria-security-idUSKBN1AC30B\n",
      "downloading: http://feeds.reuters.com/~r/Reuters/worldNews/~3/jUGQkMqtkW8/us-poland-politics-judiciary-kaczynski-idUSKBN1AC2R4\n",
      "downloading: http://feeds.reuters.com/~r/Reuters/worldNews/~3/WEP96rO-BgE/us-mideast-crisis-russia-syria-idUSKBN1AC1R9\n",
      "downloading: http://feeds.reuters.com/~r/Reuters/worldNews/~3/Yt63vK5mh4Y/us-venezuela-politics-idUSKBN1AC1S9\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Michel Rose Paris Jack Stubbs Moscow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-285-77aa41f045f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_ner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-284-fe3eb88b1568>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mupdate_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_entities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-281-60857e69f2b0>\u001b[0m in \u001b[0;36mupdate_single\u001b[1;34m(extracted)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupdate_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextracted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mextracted\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextracted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Michel Rose Paris Jack Stubbs Moscow'"
     ]
    }
   ],
   "source": [
    "update(news_loader.for_ner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
