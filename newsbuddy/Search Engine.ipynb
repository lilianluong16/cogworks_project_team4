{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SearchEngine():\n",
    "    def __init__(self):\n",
    "        # Dict[str, str]: maps document id to original/raw text\n",
    "        self.raw_text = {}\n",
    "        \n",
    "        # Dict[str, Counter]: maps document id to term vector (counts of terms in document)\n",
    "        self.term_vectors = {}\n",
    "        \n",
    "        # Counter: maps term to count of how many documents contain term\n",
    "        self.doc_freq = Counter()\n",
    "        \n",
    "        # Dict[str, set]: maps term to set of ids of documents that contain term\n",
    "        self.inverted_index = defaultdict(set)\n",
    "        \n",
    "        self.filtered_tokens = {}\n",
    "        self.unfiltered_tokens = {}\n",
    "        \n",
    "    \n",
    "    def filtered_tokenize(self, text):\n",
    "        \"\"\" Tokenizes the text, filters out the punctuation, and converts\n",
    "            text to lower case.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            text: str\n",
    "                The text of the document to be tokenized.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            the_tokens: list[str]\n",
    "                A list of tokenized words.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        the_tokens = [token.lower() for token in tokens if not token in string.punctuation]\n",
    "        \n",
    "        return the_tokens\n",
    "    \n",
    "    def unfiltered_tokenize(self, text):\n",
    "        \"\"\" Tokenizes the text, but keeps the punctuation and case of the original\n",
    "            text\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            text: str\n",
    "                The text of the document to be tokenized.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            the_tokens: list[str]\n",
    "                A list of tokenized words.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        the_tokens = word_tokenize(text)\n",
    "        return the_tokens\n",
    "    \n",
    "    def add(self, id, text, ftokens, uftokens):\n",
    "        \"\"\" Adds document to index. Stores the filtered and unfiltered tokens\n",
    "            as attributes of that document\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            id: str\n",
    "                A unique identifier for the document to add, e.g., the URL of a webpage.\n",
    "            text: str\n",
    "                The text of the document to be indexed.\n",
    "            ftokens: list\n",
    "                The list of tokenized words WITH their punctuation removed\n",
    "            uftokens: list\n",
    "                The list of tokenized words WITHOUT their punctuation removed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.filtered_tokens[id] = ftokens\n",
    "        self.unfiltered_tokens[id] = uftokens\n",
    "        # check if document already in collection and throw exception if it is\n",
    "        if id in self.raw_text:\n",
    "            raise RuntimeError(\"document with id [\" + id + \"] already indexed.\")\n",
    "        \n",
    "        # store raw text for this doc id\n",
    "        self.raw_text[id] = text\n",
    "\n",
    "        \n",
    "        # create term vector for document (a Counter over tokens)\n",
    "        term_vector = Counter(ftokens)\n",
    "        \n",
    "        # store term vector for this doc id\n",
    "        self.term_vectors[id] = term_vector\n",
    "        \n",
    "        # update inverted index by adding doc id to each term's set of ids\n",
    "        for term in term_vector.keys():\n",
    "            self.inverted_index[term].add(id)\n",
    "        \n",
    "        # update document frequencies for terms found in this doc\n",
    "        # i.e., counts should increase by 1 for each (unique) term in term vector\n",
    "        self.doc_freq.update(term_vector.keys())\n",
    "    \n",
    "    def remove(self, id):\n",
    "        \"\"\" Removes document from index.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            id: str\n",
    "                The identifier of the document to remove from the index.\n",
    "        \"\"\"\n",
    "        # check if document doesn't exists and throw exception if it doesn't\n",
    "        if not id in self.raw_text:\n",
    "            raise KeyError(\"document with id [\" + id + \"] not found in index.\")\n",
    "\n",
    "        # remove raw text for this document\n",
    "        del self.raw_text[id]\n",
    "        \n",
    "        # remove filtered tokens for this document\n",
    "        del self.filtered_tokens[id]\n",
    "        \n",
    "        \n",
    "        # remove unfiltered tokens for this document\n",
    "        del self.unfiltered_tokens[id]\n",
    "        \n",
    "        # update document frequencies for terms found in this doc\n",
    "        # i.e., counts should decrease by 1 for each (unique) term in term vector\n",
    "        self.doc_freq.subtract(self.term_vectors[id].keys())\n",
    "\n",
    "        # update inverted index by removing doc id from each term's set of ids\n",
    "        for term in self.term_vectors[id].keys():\n",
    "            self.inverted_index[term].remove(id)\n",
    "\n",
    "        # remove term vector for this doc\n",
    "        del self.term_vectors[id]\n",
    "\n",
    "    def get_text(self, id):\n",
    "        \"\"\" Returns the original (raw) text of a document.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            id: str\n",
    "                The identifier of the document to return.\n",
    "        \"\"\"\n",
    "        # check if document exists and throw exception if so\n",
    "        if not id in self.raw_text:\n",
    "            raise KeyError(\"document with id [\" + id + \"] not found in index.\")\n",
    "            \n",
    "        return self.raw_text[id]\n",
    "    \n",
    "    def num_docs(self):\n",
    "        \"\"\" Returns the current number of documents in index. \n",
    "        \"\"\"\n",
    "        return len(self.raw_text)\n",
    "    \n",
    "    \n",
    "    def get_ftokens_from_docid(self, id):\n",
    "        \"\"\" Returns a list of filtered tokens (with punctuation removed) for the document \n",
    "            specified by the doc id. \n",
    "        \"\"\"\n",
    "        return self.filtered_tokens[id]\n",
    "    \n",
    "    def get_uftokens_from_docid(self, id):\n",
    "        \"\"\" Returns a list of unfiltered tokens (without punctuation removed) for the document \n",
    "            specified by the doc id. \n",
    "        \"\"\"\n",
    "        return self.unfiltered_tokens[id]\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    #  matching\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    def get_matches_term(self, term):\n",
    "        \"\"\" Returns ids of documents that contain term.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            term: str\n",
    "                A single token, e.g., \"cat\" to match on.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            set(str)\n",
    "                A set of ids of documents that contain term.\n",
    "        \"\"\"\n",
    "        # note: term needs to be lowercased so can match output of tokenizer\n",
    "        # look up term in inverted index\n",
    "        return self.inverted_index[term.lower()]\n",
    "\n",
    "    def get_matches_OR(self, terms):\n",
    "        \"\"\" Returns set of documents that contain at least one of the specified terms.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            terms: iterable(str)\n",
    "                An iterable of terms to match on, e.g., [\"cat\", \"hat\"].\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            set(str)\n",
    "                A set of ids of documents that contain at least one of the term.\n",
    "        \"\"\"\n",
    "        # initialize set of ids to empty set\n",
    "        ids = set()\n",
    "        \n",
    "        # union ids with sets of ids matching any of the terms\n",
    "        for term in terms:\n",
    "            ids.update(self.inverted_index[term])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def get_matches_AND(self, terms):\n",
    "        \"\"\" Returns set of documents that contain all of the specified terms.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            terms: iterable(str)\n",
    "                An iterable of terms to match on, e.g., [\"cat\", \"hat\"].\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            set(str)\n",
    "                A set of ids of documents that contain each term.\n",
    "        \"\"\" \n",
    "        # initialize set of ids to those that match first term\n",
    "        ids = self.inverted_index[terms[0]]\n",
    "        \n",
    "        # intersect with sets of ids matching rest of terms\n",
    "        for term in terms[1:]:\n",
    "            ids = ids.intersection(self.inverted_index[term])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def get_matches_NOT(self, terms):\n",
    "        \"\"\" Returns set of documents that don't contain any of the specified terms.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            terms: iterable(str)\n",
    "                An iterable of terms to avoid, e.g., [\"cat\", \"hat\"].\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            set(str)\n",
    "                A set of ids of documents that don't contain any of the terms.\n",
    "        \"\"\"\n",
    "        # initialize set of ids to all ids\n",
    "        ids = set(self.raw_text.keys())\n",
    "        \n",
    "        # subtract ids of docs that match any of the terms\n",
    "        for term in terms:\n",
    "            ids = ids.difference(self.inverted_index[term])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    #  scoring\n",
    "    # ------------------------------------------------------------------------\n",
    "        \n",
    "    def idf(self, term):\n",
    "        \"\"\" Returns current inverse document frequency weight for a specified term.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            term: str\n",
    "                A term.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            float\n",
    "                The value idf(t, D) as defined above.\n",
    "        \"\"\" \n",
    "        return np.log10(self.num_docs() / (1.0 + self.doc_freq[term]))\n",
    "    \n",
    "    def dot_product(self, tv1, tv2):\n",
    "        \"\"\" Returns dot product between two term vectors (including idf weighting).\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            tv1: Counter\n",
    "                A Counter that contains term frequencies for terms in document 1.\n",
    "            tv2: Counter\n",
    "                A Counter that contains term frequencies for terms in document 2.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            float\n",
    "                The dot product of documents 1 and 2 as defined above.\n",
    "        \"\"\"\n",
    "        # iterate over terms of one document\n",
    "        # if term is also in other document, then add their product (tfidf(t,d1) * tfidf(t,d2)) \n",
    "        # to a running total\n",
    "        result = 0.0\n",
    "        for term in tv1.keys():\n",
    "            if term in tv2:\n",
    "                result += tv1[term] * tv2[term] * self.idf(term)**2\n",
    "        return result\n",
    "    \n",
    "    def length_of_doc(self, tv):\n",
    "        \"\"\" Returns the length of a document (including idf weighting).\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            tv: Counter\n",
    "                A Counter that contains term frequencies for terms in the document.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            float\n",
    "                The length of the document as defined above.\n",
    "        \"\"\"\n",
    "        result = 0.0\n",
    "        for term in tv:\n",
    "            result += (tv[term] * self.idf(term))**2\n",
    "        result = result**0.5\n",
    "        return result\n",
    "    \n",
    "    def cosine_similarity(self, tv1, tv2):\n",
    "        \"\"\" Returns the cosine similarity (including idf weighting).\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            tv1: Counter\n",
    "                A Counter that contains term frequencies for terms in document 1.\n",
    "            tv2: Counter\n",
    "                A Counter that contains term frequencies for terms in document 2.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            float\n",
    "                The cosine similarity of documents 1 and 2 as defined above.\n",
    "        \"\"\"\n",
    "        return self.dot_product(tv1, tv2) / max(1e-7,(self.length(tv1) * self.length(tv2)))\n",
    "        \n",
    "    \n",
    "    def add_all(self, input_list):\n",
    "    \n",
    "        raw_texts, f_tokens, uf_tokens = zip(*input_list)\n",
    "    \n",
    "    \n",
    "        count = 1\n",
    "        for i in range(len(raw_texts)):\n",
    "        \n",
    "            self.add(str(count),raw_texts[i],f_tokens[i], uf_tokens[i])\n",
    "            \n",
    "            count +=1\n",
    "            \n",
    "            \n",
    "    def first_sent_highest_doc(self, q, k=1):\n",
    "        \"\"\" Finds the top document that contains that most occurences of the query (q)\n",
    "            and returns the first sentence of that document.\n",
    "            \n",
    "            *Raises an Exception if the query is not found in any of the documents the\n",
    "            search engine contains*\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            q: str\n",
    "                A string containing the query words to match on, e.g., \"cat hat\".\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            the_first_sent: str\n",
    "                A first sentence of the highest ranked document.\n",
    "                \n",
    "        \"\"\"\n",
    "        # tokenize query\n",
    "        # note: it's very important to tokenize the same way the documents were so that matching will work\n",
    "        query_tokens = self.filtered_tokenize(q)\n",
    "        \n",
    "        # get matches\n",
    "        # just support OR for now...\n",
    "        ids = self.get_matches_OR(query_tokens)\n",
    "        \n",
    "        \n",
    "        # raises an exception if the query is not found in any of the documents\n",
    "        if len(ids) == 0:\n",
    "            raise Exception(\"Sorry, no matches were found with the query \\' \" + q + \" \\' \")\n",
    "                \n",
    "        # convert query to a term vector (Counter over tokens)\n",
    "        query_tv = Counter(query_tokens)\n",
    "        \n",
    "        # score each match by computing cosine similarity between query and document\n",
    "        scores = [(id, self.cosine_similarity(query_tv, self.term_vectors[id])) for id in ids]\n",
    "        scores = sorted(scores, key=lambda t: t[1], reverse=True)\n",
    "\n",
    "        # highest ranked document's id\n",
    "            \n",
    "        the_highest_id = scores[0][0]\n",
    "        \n",
    "        # converting the document's id to a raw text string\n",
    "        the_highest_text = self.get_text(the_highest_id)\n",
    "        \n",
    "        \n",
    "        # tokenizing the sentences in the highest ranked document and returing the\n",
    "        #first element of that list, which represents the first sentence of that document\n",
    "\n",
    "        the_first_sent = sent_tokenize(the_highest_text)[0]\n",
    "        \n",
    "        return the_first_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                               TESTS for Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Filtered Tokenization Method\n",
    "\n",
    "\n",
    "    def f_tokenize(text):\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        return [token.lower() for token in tokens if not token in string.punctuation]\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ### Unfiltered Tokenization Method\n",
    " \n",
    "    def uf_tokenize(text):\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black Hole Document 1\n",
    "\n",
    "black_hole_1 = '''A black hole is a region of spacetime exhibiting such strong gravitational effects that \n",
    "      nothing—not even particles and electromagnetic radiation such as light—can escape from \n",
    "      inside it.The theory of general relativity predicts that a sufficiently compact mass \n",
    "      can deform spacetime to form a black hole. The boundary of the region from which no \n",
    "      escape is possible is called the event horizon. Although the event horizon has an enormous \n",
    "      effect on the fate and circumstances of an object crossing it, no locally detectable features \n",
    "      appear to be observed. In many ways a black hole acts like an ideal black body, as it reflects \n",
    "      no light. Moreover, quantum field theory in curved spacetime predicts that event horizons \n",
    "      emit Hawking radiation, with the same spectrum as a black body of a temperature inversely \n",
    "      proportional to its mass. This temperature is on the order of billionths of a kelvin for black \n",
    "      holes of stellar mass, making it essentially impossible to observe.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Black Hole Document 2\n",
    "\n",
    "black_hole_2 = '''Black holes of stellar mass are expected to form when very massive stars collapse at the\n",
    "        end of their life cycle. After a black hole has formed, it can continue to grow by absorbing \n",
    "        mass from its surroundings. By absorbing other stars and merging with other black holes, \n",
    "        supermassive black holes of millions of solar masses may form. There is general consensus \n",
    "        that supermassive black holes exist in the centers of most galaxies.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Black Hole Document 3\n",
    "\n",
    "black_hole_3 = '''Despite its invisible interior, the presence of a black hole can be inferred through its \n",
    "        interaction with other matter and with electromagnetic radiation such as visible light. \n",
    "        Matter that falls onto a black hole can form an external accretion disk heated by friction, \n",
    "        forming some of the brightest objects in the universe. If there are other stars orbiting a \n",
    "        black hole, their orbits can be used to determine the black hole's mass and location. Such \n",
    "        observations can be used to exclude possible alternatives such as neutron stars. In this way, \n",
    "        astronomers have identified numerous stellar black hole candidates in binary systems, and \n",
    "        established that the radio source known as Sagittarius A, at the core of our own Milky Way \n",
    "        galaxy, contains a supermassive black hole of about 4.3 million solar masses. '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Black Hole Document 4\n",
    "\n",
    "black_hole_4 = '''When an object falls into a black hole, any information about the shape of the object or \n",
    "        distribution of charge on it is evenly distributed along the horizon of the black hole, \n",
    "        and is lost to outside observers. The behavior of the horizon in this situation is a \n",
    "        dissipative system that is closely analogous to that of a conductive stretchy membrane with \n",
    "        friction and electrical resistance—the membrane paradigm. This is different from other \n",
    "        field theories such as electromagnetism, which do not have any friction or resistivity at \n",
    "        the microscopic level, because they are time-reversible. Because a black hole eventually \n",
    "        achieves a stable state with only three parameters, there is no way to avoid losing \n",
    "        information about the initial conditions: the gravitational and electric fields of a black \n",
    "        hole give very little information about what went in. The information that is lost includes \n",
    "        every quantity that cannot be measured far away from the black hole horizon, including \n",
    "        approximately conserved quantum numbers such as the total baryon number and lepton number. \n",
    "        This behavior is so puzzling that it has been called the black hole information loss \n",
    "        paradox.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Black Hole Document 5\n",
    "\n",
    "black_hole_5 = '''The defining feature of a black hole is the appearance of an event horizon—a boundary in \n",
    "    spacetime through which matter and light can only pass inward towards the mass of the black \n",
    "    hole. Nothing, not even light, can escape from inside the event horizon. The event horizon is \n",
    "    referred to as such because if an event occurs within the boundary, information from that event \n",
    "    cannot reach an outside observer, making it impossible to determine if such an event occurred.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generating Pseudo Input\n",
    "\n",
    "````python\n",
    "f1 = f_tokenize(black_hole_1)\n",
    "f2 = f_tokenize(black_hole_2)\n",
    "f3 = f_tokenize(black_hole_3)\n",
    "f4 = f_tokenize(black_hole_4)\n",
    "f5 = f_tokenize(black_hole_5)\n",
    "\n",
    "uf1 = uf_tokenize(black_hole_1)\n",
    "uf2 = uf_tokenize(black_hole_2)\n",
    "uf3 = uf_tokenize(black_hole_3)\n",
    "uf4 = uf_tokenize(black_hole_4)\n",
    "uf5 = uf_tokenize(black_hole_5)\n",
    "\n",
    "l = [(black_hole_1,f1,uf1), (black_hole_2,f2,uf2), (black_hole_3,f3,uf3), (black_hole_4,f4,uf4),(black_hole_5,f5,uf5)]\n",
    "\n",
    "````\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Adding Documents to Search Engine\n",
    "````python\n",
    "Google = SearchEngine()\n",
    "Google.add_all(l)\n",
    "\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
